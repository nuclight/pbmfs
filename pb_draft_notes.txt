10.03.24
зациклился на идее файловой системы в raw разделе с sqlite внутри для всей меты - как её хранить? бесполезно, но *** ********, *** ** ****** * ********...
вообще исходно мысль возникла от потребности в новой DVCS, и смеси разных репозиториев (IPFS?), в таком виде польза возможна
===
11.03.24
вечером опять по идее fs с sqlite, видимо традиционные indirect-блоки не очень, надо overflow maps на моменты роста в транзакции из запомненных
в памяти списков свободных блоков... тогда это ограничивает на только один процесс писателя?
===
15.03.24
проблема повреждений: делаем recovery-сектора как XOR в RAR для каждого метафайла; чтоб больше одного сектора восстановить, можно еще разбить раздел на groups и в каждой тоже иметь XOR-сектор на чисто её диапазон
===
16.03.24
всю неделю думал, как представлять в базе, вдруг понял, что проблема найти
саму базу-то, bootstrap, не решена
===
17.03.24
курил доки по NTFS, смотрел вывод ntfsinfo -vi 46106 у исошки на 4 гига и выяснилась жесть: для
длинного файл в base FILE record живёт runlist от attribute $ATTRIBUTE_LIST,
в его кластерах на диске (!) - то есть вне MFT - живут списки, записи $DATA со
Starting VCN на всю запись, ссылающиеся на номера extension FILE record в MFT
каждая, а уже в каждой из них находится runlist собственно файлов на диске.
Может, сделать аналог MFT, а вместо ntfs-ных атрибутов сделать CBOR? типа мап
с ключом L - для листьев, конечные блоки, M ссылаются на записи в MFT,
U ссылаются на записи в MFT для M... А каталог можно CBOR-массивом номеров
base record сделать, тогда первые 23 для файлов, 24-31 для grow самой MFT.
===
18.03.24
********** **** *****, ранлисты сложно, а что если сделать как в inode? типа
первый мегабайт описывается прямо в base record, и это как фрагменты в UFS2,
по 32 Кб скажем, в первой indirect-записи чуть места на заголовок и остаток
на 63 указателя описывает суммарно уже первые 64 Мб файла, т.е. WAL/журнал
будут увеличиваться по 32 Кб и лишь потом выделяться кусками по 1M, чтоб
меньше потерь. Проблема тогда со следующим уровнем индиректов, в каждой записи
же на заголовок место надо, некрасиво получается по неровности, и еще более - без
экстентов код простой, но дофига места потратится...

Следующая идея: что, если всё-таки сделать для метафайлов маленькую отдельную
базу? Но всё равно надо её как-то найти. И её журнал. Тут две идеи:
* маленький MFT с фиксированным числом base чисто под неё
* резервировать на каждой странице допустим 8 байт указателя
  - связный список долго
  - в первой полосе по 1 Мб каждая страница на полосы по 1 Мб?
  - а про журнал где хранить? 
===
20.03.24
начал читать про ReiserFS, оказывается, Reiser3 уже и есть одно большое B-Tree!
https://reiser4.wiki.kernel.org/index.php/V4 :
<<Unix differs from Multics, in that Multics defined a file to be a sequence of elements (the elements could be bytes, directory entries, or something else....), while Unix defines a file to be purely a sequence of bytes. In Multics directories were then considered to be a particular type of file which was a sequence of directory entries. For many years, all implementations of Unix directories were as sequences of bytes, and the notion of location within a Unix directory is tied not to a name as you might expect, but to a byte offset within the directory.>> -- а вот это полезно для VCS

<<The problem is that one is using a byte offset to represent a location whose true meaning is not a byte offset but a directory entry, and doing so for a particular file in a system which meaningfully names that file not by byte offset within the directory but by filename. Various efforts are being made in the Unix community to pretend that this byte offset is something more general than a byte offset, and they often try to do so without increasing the size used to store the thing which they pretend is not a byte offset. Since byte offsets are normally smaller than filenames are allowed to be, the result is ugliness and pathetic kludges. Trust me that you would rather not know about the details of those kludges unless you absolutely have to, and let me say no more>>

<< Can We Get By Using Just Files and Directories

(Composing Streams And Attributes From Files And Directories)?

In Gnu/Linux we have files, directories, and attributes. In NTFS they also have streams. Since Samba is important to Gnu/Linux, there frequently are requests that we add streams to ReiserFS. There are also requests that we add more and more different kinds of attributes using more and more different APIs. Can we do everything that can be done with {files, directories, attributes, streams} using just {files, directories}? I say yes--if we make files and directories more powerful and flexible. I hope that by the end of reading this you will agree.

Let us have two basic objects. A file is a sequence of bytes that has a name. A directory is a name space mapping names to a set of objects "within" the directory. We connect these directory name spaces such that one can use compound names whose subcomponents are separated by a delimiter '/'. What is missing from files and directories now that attributes and streams offer?

In ReiserFS 3, there exist file attributes. File attributes are out-of-band data describing the sequence of bytes which is the file. For example, the permissions defining who can access a file, or the last modification time, are file attributes. File attributes have their own API; creating new file attributes creates new code complexity and compatibility issues galore. ACLs are one example of new file attributes users want.

Since in Reiser4 files can also be directories, we can implement traditional file attributes as simply files. To access a file attribute, one need merely name the file, followed by a '/', followed by an attribute name. That is: a traditional file will be implemented to possess some of the features of a directory; it will contains files within the directory corresponding to file attributes which you can access by their names; and it will contain a file body which is what you access when you name the "directory" rather than the file.

Unix currently has a variety of attributes that are distinct from files (ACLS, permissions, timestamps, other mostly security related attributes, ...). This
is because a variety of people needed this feature and that, and there was no infrastructure that would allow implementing the features as fully orthogonal
features that could be applied to any file. Reiser4 will create that infrastructure.>> -- а вот и оно! что нужно для VCS.
кстати, в Солярке же openat() родился, идея кажется в точности такая же, #todo перечитать её маны

<< List Of Features Needed To Get Attribute And Stream Functionality From Files And Directories
*    api efficient for small files
*    efficient storage for small files
*    plugins, including plugins that can compress a file serving as an attribute into a single bit
*    files that also act as directories when accessed as directories
*    inheritance (includes file aggregation)
*    constraints
*    transactions
*    hidden directory entries 
Each of these additional features is a feature that would benefit the filesystem. So we add them in v4.>> -- и это же нужно в любой системе контроля версий.
===
21.03.24
<<Procrastination Leads To Wiser Decisions: Allocate on Flush>>

Vadim Goncharov, [21 Mar 2024 18:17:14]
https://habr.com/ru/articles/559014/ о, вот то интервью, откуда цитата в нашем стикерпаке

Vadim Goncharov, [21 Mar 2024 18:18:27]
> По HAMMERу: читал статью от создателя. Не заинтересовало. Опять же, B-деревья. Эта структура данных безнадёжно устарела. Мы отказались от неё ещё в прошлом веке.

но как, если в Reiser4 тоже дерево?

Vadim Goncharov, [21 Mar 2024 18:23:58]
> Операция добавления устройства C не откатилась, и если вы сейчас удалите устройство C из компьютера, то это закорраптит ваши данные, так что перед удалением вам нужно будет сначала провести дорогостоящую операцию удаления устройства из логического тома с перебалансировкой, которая раскидает все данные с устройства C на устройства A и B. А вот если бы ваша ФС поддерживала глобальные снимки, такая перебалансировка бы не потребовалась, и после моментального отката к S вы бы могли смело удалить устройство C из компьютера

занятно

Yuri Myasoedov, [21 Mar 2024 18:25:45]
C-деревья?

Vadim Goncharov, [21 Mar 2024 18:26:49]
> Следующее, чему стоит поучиться локальным ФС у сетевых - это хранить метаданные на отдельных устройствах точно так же, как сетевые ФС хранят их на отдельных машинах (так называемые метадата-серверы). Есть приложения, работающие в основном с метаданными, и эти приложения можно значительно ускорить, разместив метаданные на дорогостоящих высокопроизводительных накопителях. Со связкой ФС+LVM проявить такую избирательность вам не удастся: LVM не знает, что там на блоке, который вы ему передали (данные там или же метаданные). От реализации в ФС собственного низкоуровневого LVM большого выигрыша по сравнению со связкой ФС+LVM вы не получите, а вот что у вас получится очень хорошо - так это захламить ФС так, что потом станет невозможно работать с её кодом. ZFS и Btrfs, поспешившие с виртуальными девайсами, - это всё наглядные примеры того, как layering violation убивает систему в архитектурном плане.Итак, к чему я всё это? Да к тому, что не нужно городить в файловой системе свой собственный низкоуровневый LVM. Вместо этого нужно агрегировать устройства в логические тома на высоком уровне, как это делают некоторые сетевые ФС с разными машинами (storage nodes). Правда, делают они это отвратительно по причине применения плохих алгоритмов.

всех обосрал!

Vadim Goncharov, [21 Mar 2024 18:27:18]
я подозреваю, что речь о B-tree vs B+tree vs B*tree

Vadim Goncharov, [21 Mar 2024 18:46:36]
Если говорить только в терминах интерфейсов и плагинов (модулей), которые их реализуют, то не всё. Но если ввести ещё и отношения на этих интерфейсах, то помимо всего прочего у вас возникнут понятия высших полиморфизмов, которыми уже можно обойтись. Представьте, что вы гипотетически заморозили объектно-ориентированную систему времени выполнения, поменяли значение instruction pointer, чтобы он указывал на другой плагин, который реализует тот же интерфейс X, а потом разморозили систему, так чтобы она продолжила выполнение. Если при этом конечный пользователь не заметит такой "подмены", то мы говорим, что система обладает полиморфизмом нулевого порядка в интерфейсе X (или система гетерогенна в интерфейсе X, что то же самое). Если теперь у вас не просто набор интерфейсов, а ещё имеются и отношения на них (граф интерфейсов), то можно ввести полиморфизмы и более высоких порядков, которые будут характеризовать гетерогенность системы уже в "окрестности" какого-либо интерфейса. Такую классификацию я когда-то давно ввёл, но опубликовать, к сожалению, так и не получилось. Так вот, при помощи плагинов и таких вот высших полиморфизмов можно описать любую известную фичу, а также "предсказать" те, которые никогда даже не упоминались. Строго доказать это мне не удалось, но и контрпримера я тоже пока не знаю. Вообще, вопрос этот напомнил мне "Эрлангенскую Программу" Феликса Клейна. Он в своё время пытался представить всю геометрию разделом алгебры (конкретно, теории групп).

Vadim Goncharov, [21 Mar 2024 18:49:09]
И, наконец, появились гетерогенные логические тома, предлагающие всё то, чего ZFS, Btrfs, block layer, а также связки FS+LVM в принципе дать не могут - это параллельное масштабирование, O(1)-аллокатор дисковых адресов, прозрачная миграцией данных между подтомами. Для последней также имеется пользовательский интерфейс. Теперь наиболее "горячие" данные вы без труда можете переместить на самый высокопроизводительный накопитель вашего тома. Кроме того, имеется возможность экстренно сбрасывать на такой накопитель любые грязные страницы, и, тем самым, значительно ускорить приложения, часто вызывающие fsync(2). Отмечу, что функциональность block layer, называемая bcache, совершенно не предоставляет такой свободы действий. Новые логические тома основаны на моих алгоритмах (есть соостветствующие патенты). Софт уже достаточно стабилен, вполне можно попробовать, замерить производительность и т.п. Единственное неудобство - пока нужно вручную обновлять конфигурацию тома и где-то ёё хранить.

Реализовать свои задумки мне удалось пока что процентов на 10. Однако, удалось то, что я считал наиболее трудным - это "подружить" логические тома с флаш-процедурой, которая выполняет все отложенные действия в reiser4. Это всё пока в экспериментальной ветке "format41".

— Проходит ли Reiser4 тесты xfstests?

По крайней мере, у меня прошла, когда я готовил последний релиз.

Vadim Goncharov, [21 Mar 2024 18:49:54]
> случае провала проверки контрольной суммы какого-либо блока Reiser4 немедленно считывает соответствующий блок с девайса-реплики. Заметьте, что ZFS и Btrfs так не могут: не позволяет дизайн. Там вы должны запустить специальный фоновый сканирующий процесс под названием "скраб" и ждать, когда он доберётся до проблемного блока. Такие мероприятия программисты образно называют "костылями".

бля, ну это пиздёж, скруб в zfs делает не это

Vadim Goncharov, [21 Mar 2024 18:50:52]
— Если с Reiser4 в Linux'е так ничего и не получится, хотелось бы предложить ФС для FreeBSD (цитата из прошлого интервью: «…FreeBSD … имеет академические корни… А это означает, что с большой долей вероятности мы найдём с разработчиками общий язык»)?

Итак, как мы только что выяснили, с Линуксом у нас всё уже прекрасно получилось: под него есть отдельный работающий порт Reiser4 в виде мастер-бранча нашего репозитория. Про FreeBSD я и не забыл! Предлагайте! Готов плотно поработать c теми, кто хорошо знает внутренности FreeBSD. Кстати: что мне очень нравится в их сообществе - там решения принимаются обновляемым советом независимых экспертов, не имеющим ничего общего с губонадувательством одной бессменной персоны.

Vadim Goncharov, [21 Mar 2024 18:53:32]
он не хочет:

В апстриме давно уже нет никакого развития файловых систем. Создаётся лишь видимость такового. Разработчики локальных ФС упёрлись в проблемы связанные с неудачным дизайном. Здесь нужно сделать оговорку. Т.н "хранение", "вылизывание" и портирование кода я за развитие и разработку не считаю. А недоразумение под названием "Btrfs" к разработкам не причисляю по причинам, которые я уже объяснил. Каждый патч лишь усугубляет её проблемы. Ну, и всегда находятся разного рода "евангелисты", у которых "всё работает". В основном, это школьники и студенты, прогуливающие лекции. Вы только представьте: у него работает, а у профессора нет. Это какой же выброс адреналина! Наибольший же вред с моей точки зрения приносят "умельцы", бросившиеся с энтузиазмом "привинчивать" чудо-фичи Btrfs к всевозможным прослойкам типа systemd, docker, и т.п. - это уже напоминает метастазы.

Давайте теперь попробуем сделать прогноз на пять-десять лет. Что мы будем делать в Reiser4 я вкратце уже перечислил. Основным вызовом для разработчиков локальных ФС из апстрима станет (да, уже стало) невозможность заниматься приличным делом за зарплату. Без каких-либо идей в области хранения данных они будут продолжать пытаться патчить эти несчастные VFS, XFS и ext4. Особенно комично на этом фоне выглядит ситуация с VFS, напоминающая остервенелую модернизацию ресторана в котором поваров нет, и не предвидится. Теперь код VFS безо всяких условий залочивает одновременно несколько страниц памяти и предлагает нижележащей ФС оперировать над ними. Это было введено для повышения производительности Ext4 на операциях удаления, но, как нетрудно понять, такой одновременный лок совершенно несовместим с продвинутыми транзакционными моделями. То есть, добавить поддержку какой-то умной ФС в ядре вы уже просто так не сможете. Я не знаю, как дело обстоит в остальных областях Linux, но что касается файловых систем, какое-либо развитие здесь вряд ли совместимо с политикой, проводимой Торвальдсом на деле (академические проекты изгоняются, а мошенникам, не имеющим понятия, что такое B-дерево, выдаются бесконечные кредиты доверия). Поэтому тут был взят курс на медленное загнивание. Его, конечно же, изо всех сил будут пытаться выдать за "развитие". Далее, "хранители" файловых систем, поняв, что на одном лишь "хранении" много не заработаешь, будут пробовать себя в более прибыльном бизнесе. Это, как правило, распределенные файловые системы и виртуализация. Возможно, куда-то ещё будут портировать модную ZFS там, где её ещё нет. Но она, как и все ФС из апстрима, напоминает новогоднюю ёлку: если сверху ещё что-то из мелочи повесить можно, то глубже уже не подлезешь. Я допускаю, что построить серьёзную энтерпрайз-систему на базе ZFS можно, но поскольку мы сейчас обсуждаем будущее, то мне остаётся с сожалением констатировать, что ZFS в этом плане безнадёжна: своими виртуальными девайсами ребята перекрыли себе и будущим поколениям кислород для дальнейших разработок. ZFS - это вчерашний день. А ext4 и XFS - уже даже не позавчерашний.

Vadim Goncharov, [21 Mar 2024 18:53:32]
Отдельно стоит сказать про нашумевшее понятие "Linux file system of next generation". Это полностью политический и маркетинговый проект, созданный для возможности, так скажем, "столбить будущее файловых систем" в Linux за конкретными персонажами. Дело в том, что это раньше Linux был "just for fun". А сейчас это прежде всего машина для зарабатывания денег. Они делаются на всём, на чём только можно. К примеру, создать хороший программный продукт очень трудно, но смышленые "разработчики" давно уже сообразили, что напрягаться-то вовсе и не нужно: успешно продавать можно и несуществующий софт, анонсированный и распиаренный на всевозможных публичных мероприятиях - главное, чтобы в презентационных слайдах было побольше "фич". Файловые системы подходят для этого как нельзя лучше, ибо на результат можно смело выторговывать лет десять. Ну, а если кто-то потом будет сетовать на отсутствие этого самого результата, то он же в файловых системах просто ничего не смыслит! Это напоминает финансовую пирамиду: на вершине располагаются заварившие эту кашу авантюристы, и те немногие, кому "повезло": они "сняли дивиденды", т.е. получили деньги на разработку, устроились менеджерами на высокооплачиваемую работу, "засветились" на конференциях, и т.п. Далее идут те, кому "не повезло": они будут подсчитывать убытки, расхлебывать последствия развертывания в продакшн непригодного программного продукта ", и т.д. Их гораздо больше. Ну, и в основании пирамиды - огромная масса разработчиков, "пилящих" никчёмный код. Они - в самом большом проигрыше, ибо впустую потраченное время не вернёшь. Такие пирамиды чрезвычайно выгодны Торвальдсу и его приближенным. И чем этих пирамид больше - тем для них лучше. Для подпитки таких пирамид в ядро может быть принято всё что угодно. Разумеется, на публике они утверждают обратное. Но я сужу не по словам а по поступкам.

Так что, "будущее файловых систем в Линукс" - это очередной сильно распиаренный, но мало пригодный к использованию софт. После Btrfs с большой вероятностью место такого "будущего" займёт Bcachefs, представляющая собой ещё одну попытку скрестить Linux block layer с файловой системой (дурной пример заразителен). И что характерно: там те же проблемы, что и в Btrfs. Я давно это подозревал, а потом как-то не удержаляся и заглянул в код - так и есть! Авторы Bcachefs и Btrfs, создавая свои ФС, активно пользовались чужими исходниками, мало что в них понимая. Ситуация очень напоминает детскую игру "испорченный телефон". И я примерно представляю, как будет происходить включение этого кода в ядро. Собственно "грабли" никто не увидит (на них все будут наступать потом). После многочисленных придирок к стилю кода кода, обвинению в несуществующих нарушениях, и пр. будет делаться заключение о "лояльности" автора, о том, насколько он хорошо "взаимодействует" с остальными разработчиками, и как успешно всё это потом можно будет продавать корпорациям. Конечный же результат никого не заинтересует. Лет двадцать назад, может быть, бы и заинтересовал, но сейчас вопросы ставятся по-другому: получится ли это раскрутить так, чтобы ближайший десяток лет определённые люди оказались трудоустроены. А задаваться вопросом о конечном результате, увы, не принято.

-- отсюда вывод: выносить sqlite-базы наружу раздела при возможности, и только для
случая внутри - делать мини-файловую систему, видимо по принципу аллокации
цилиндр-групп

Посмотрел в sys/ufs/ffs/fs.h, CGSIZE() занимает максимум 1 блок - внутри
битовые карты inode и данных, дальше идут сами иноды. Тогда, если хардкодим
блок mini-fs в 4096 т.е. 1 бит на 4096 байт данных в битмапе, тогда отсеки
compartments/cylinder groups размера 64 Мб (2^26): это было бы 2048 байт на
2^14 блоков по 4k максимум, но еще отводим битмап на MFT, которая будет типа
вместо инод, тогда она может занять до 8 Мб если бы все 2048 байт, и еще на
XOR-сектора надо оставить... или их пускай уже в data free bitmap?
Но главное снаружи экстенты по 64 Мб, а внутри можно собственную адресацию, но
как быть с малыми fs? делать переменного размера, допустим до 100 Мб за счет
меньше на MFT - но тогда вычисление оффсета внутри станет зависимым от
указателей из внешнего тома...

нужно посчитать, сколько максимум может занимать MFT, т.е. экстенты при
максимальной фрагментации - если max 2^56 блоков, то по 8 байт на блок?
а еще размер в секторе, если двусвязный список всех листьев делать...
===
22.03.24
struct cg {
	uint32_t cg_magic;		/* magic number */
	uint32_t cg_cgx;		/* we are the cgx'th cylinder group */
	uint16_t cg_niblk;		/* number of MFT blocks this cg */
	uint16_t cs_nifree;		/* number of free inodes */
	uint32_t cg_ndblk;		/* number of data blocks this cg */
	uint32_t cs_nbfree;		/* number of free blocks */
	uint16_t cg_freeoff;		/* (u_int8) free block map */
	uint8_t  cg_iusedoff;		/* (u_int8) used MFT map */
	uint8_t  cg_extmftoff;		/* MFT record extending CG struct, if >0 */
	uint64_t cg_time_ckhash;	/* time last written / check-hash of this cg */
	uint8_t cg_space[1];		/* space for free maps & future use */
/* actually longer */
};
-> 32 bytes

- с биткартой MFT не влезает, блин

- можно сделать запись в MFT с битмапом MFT, но тогда максимальный размер
  блока 32768:
(65536-32)/65 = 1007.75384615384615384615
1007*64*8 = 515584
1007*64*8*65536/1048576 = 32224.00000000000000000000
1007*8*512/1048576 = 3.93359375000000000000
32768*8*32768/1048576 = 8192.00000000000000000000
(32768-32)/65 = 503.63076923076923076923
===
25.03.24
можно сделать блоки и меньше 4096, если MFT-записи нумеровать не с самых
первых, а самые первые оставить на:
1) битмап
2) XOR-сектор MFT в этом отсеке
3) от 2 до 4 секторов на журнал?
4) возможные 1-2 сектора расширения вместо целого байта cg_extmftoff?
===
26.03.24
если поднять размер MFT-записи только до 1 Кб, то сложности с NTFS-стилем
sequence number updates array можно избежать - первая в начале первого
сектора, вторая в конце второго, и пространство между ними непрерывно тогда
===
27.03.24
расчеты выше по 64+1 неверны, надо так: для блока 2^B байт в отсеке может
уместиться 2^(B+3) блоков = 2^(2*B+3) байт размер отсека, и они требуют по
8 байт MFT на 1 блок, т.е. 1 блок MFT может описать 2^(B-3) блоков данных
=> 2^(B+3) / 2^(B-3) = 64 блока (размером 2^B) MFT независимо от размера отсека

04.04 можно ж не е**ть мозги дотягиванием до 128 Мб и так и оставить 126 Мб 264 Кб,
из них 256 Кб на MFT (256 бит по 1 Кб), 1 заголовочный, 1 XOR-блок
===
28.03.24
формат префикса адреса: 24328xxxhash...
2 цифры - год
1 символ - месяцы 1-9 или o=Oct, s=Nov (Scorpio), w=Dec (Winter)
2 цифры дата
как закодировать время, и нужно ли? тип хэш функции остального адреса? бранч?
ну в принципе бранч допустим одну букву даже, а если цифра - то тип артефакта,
например тикет или wiki?
вообще тип хэша можно по https://github.com/multiformats/multihash вынести
в него самого, а вот кодировать ли размер прямо в линке? типа 123m 2^64 ~5018 кодов
-> для коммитов пожалуй нет, его может быть сложно посчитать, а для конечных
   блобов всё равно другой формат, не этот

RFC 6920 https://www.iana.org/assignments/named-information/named-information.xhtml наверное лучше
===
29.03.24
zfs clones: этому просто соответствуют разные git worktree / fossil open checkouts
ну. видимо, коммит должен быть по типу манифеста в fossil, где вместо имён
файлов - пары GUID:version каждого объекта
Скорость: O(N) от числа файлов/объектов похоже не избежать, но можно сделать
процесс приемлемо быстрым, если каждая инода (объект) уже готова: в ней просто
проставляется линк на коммит, а собственно copy on write случится при
дальнейшей записи в файл когда-то потом приложением, т.е. надо просто
постоянно поддерживать иноды в хэшированном состоянии
То есть worktree/checkout для живых файлов всегда по умолчанию указывает на
иммутабельные объекты коммита, и лишь по мере записей приложениями эти иноды
мутабельны, тогда это один и тот же механизм и для rollback'ов выходит

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     | Year % 100  | Month |   Day   |Artifact Type / Branch Checksum|
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

if this is commit, then branch name checksum is taken in base36,
if this is other object, it's type is encoded as 3 lowercase letters.
How these are distinguished? See,

36^3 = 46656
26^3 = 17576
46656+17576 = 64232

so there 65536 space is divided in two parts and values above 0xfae8 are
reserved for future use. If Branch Checksum happens to consist of only
letters, then middle letter is made capital.
TBD which capital letter looks better / human readable, first? middle? last?
may be last is better as delimeter of hash?

TBD https://en.wikipedia.org/wiki/DEC_RADIX_50 (MOD40) ?

читал https://rffr.de/wp-content/uploads/manuell/file_systems_usability.pdf (записывал в notes)
- как решать проблему циклов в секции 3.7? имитировать симлинки?
- идея с одинаковыми инодами - плохая или наоборот, надо номер хардлинка кодить в ino?
===
30.03.24
dlevel в коммит как dump level - массив ссылок (длина = наш левел) на предыдущие левелы
с точки зрения удобства синхронизации, удобен WHERE art_id > $last_rcvd тогда лучше запихнуть полное время - чем, NTP?
- с разными репозиториями проблема, время ж не синхронизировано
===
31.03.24
вопрос UUID - что, если взять версию 8 из https://datatracker.ietf.org/doc/html/draft-ietf-uuidrev-rfc4122bis
и сунуть Julian day в первые 6 байт?
sqlite> SELECT julianday('2024-03-31 21:26:08');
2460401.39314815
$ perl -e '$a=2460401.39314815;say pack d=>$a' | hexdump -C
00000000  b7 ad 52 b2 78 c5 42 41                           |..R.x.BA|
$ perl -E 'say 0x4142c578b252adb7>>52'
1044
$ perl -E 'say 0x4142c578b252adb7&(2**53-1)'
780072131931575
$ perl -E 'say 0x2c578b252adb7'
80072131931575
$ perl -E 'say 0x12c578b252adb7>>(52-(1044-1023))'
2460401
sqlite> SELECT julianday('9999-03-31 21:26:11');
5373209.39318287
$ perl -e 'print pack d=>5373209.39318287' | hexdump -C 
00000000  7c e8 29 59 46 7f 54 41                           ||.)YF.TA|
$ perl -E 'say unpack d=>"\x7c\xe8\x29\x59\x46\x7f\x5f\x41"'
8256793.39318287
это ~ 25.03.17894 21:26:11 UTC
$ perl -E 'say unpack d=>"\x00\xff\xff\xff\xff\xff\x5f\x41"'
8388607.99999976
это ~ 16.02.18255 11:59 UTC
$ perl -E 'say unpack d=>"\x7c\xe8\x29\x59\x46\x7f\xff\x41"'
8454956434.61926
-- это более чем 23 миллиона лет
$ perl -E 'say unpack d=>"\x00\x00\x00\x00\x00\x00\x40\x41"'
2097152
-- 10 september 1029 proleptic
perl -E 'say unpack d=>"\x00\xff\xff\xff\xff\xff\x4f\x41"'
4194303.99999988
-- ~ 07.07.6771 

а если в идентификатор коммита тоже время поточнее засунуть?
26*36^2 = 33696
365.25*1440*10000/2^32 = 1.22459605336189270019
365.25*1440*10000-2^32 = 964632704.00
964632704/1440/365.25  = 1834.04194995817172408548
т.е. счетчик минут с 1835 до 9999 года влезет в 32 бит

ну или можно оставить Year % 100, добавить 11 бит на HH:MM, а оставшиеся 5 бит
из 32 поместить в начало - как общий формат, и про время, и про сумму, и про хэш, тогда 6 байт всего удлинение

volume is good word, как всемирная библиотека у фантастов
еще это напоминает у Telegram файлы - volume_id
тогда может быть это будет один из вариантов в #***
а в нём нужно заюзать что-то из фич FreeBSD, а если с нуля fs, то что? дедупликация блоков ZFS ?
тогда вариантов реализации получается не 2, а 3:
- полностью внутри raw block device и meta-fs для *.sqlite*
- *.sqlite* снаружи и raw block device для больших
- *.sqlite* снаружи и набор файлов на ZFS, которые будут ей дедуплицироваться
===
01.04.24
атрибут имени файла = holding relationship, а в каталоге можно еще reference в другие репы делать, тогда у их файлов не будет атрибута имени
наследование properties делать только по holding, с фильтрацией по security e.g. между репами
что, если каталог делать как SQL table? не только имя/линк, но и дата хотя бы? плюс индексы
Embedding relationships - это уже возня с атрибутом data, составным?
- нет, это просто массивы relationships, метаданные, это уже дело
  MIME-хэндлера DATA как их использовать (по индексу?)
как юзеру показать разницу между reference и embedding? как между `<img src=cat.jpg>` и `<a href=cat.jpg>`
===
02.04.24
таки что делать с распределенными коммитами?
индексы для коммитов по аналогии с каталогами для файлов? higher-order ("milestone"/release?) commits?
расширить понятие бранчей?

> *Differences With Other DVCSes*
> *Single DAG*
> Fossil keeps all check-ins on a single DAG. Branches are identified with tags. This means that check-ins can be freely moved between branches simply by altering their tags.
> Most other DVCSes maintain a separate DAG for each branch.
> *Branch Names Need Not Be Unique*
> Fossil does not require that branch names be unique, as in some VCSes, most notably Git. Just as with unnamed branches (which we call forks) Fossil resolves such ambiguities using the timestamps on the latest check-in in each branch. If you have two branches named "foo" and you say fossil update foo, you get the tip of the "foo" branch with the most recent check-in.
> This fact is helpful because it means you can reuse branch names, which is especially useful with utility branches. There are several of these in the SQLite and Fossil repositories: "broken-build," "declined," "mistake," etc. As you might guess from these names, such branch names are used in renaming the tip of one branch to shunt it off away from the mainline of that branch due to some human error. (See fossil amend and the Fossil UI check-in amendment features.)

forrest of Cthulhu? каучконосный фикус? https://pibig.info/uploads/posts/2022-12/1670420927_9-pibig-info-p-fikus-v-prirode-instagram-9.jpg
в каждом object (UUID) - его собственные именовать ревизиями, он как RCS?
если коммит ссылается на каталог, а из того пойти по UUID, то будет как git tree?
- нет, мы же не изменяем каталоги без реального изменения
- возможно, если ревизии каждого объекта имеют одинаковый адрес с коммитом

> К недостаткам Darcs можно отнести отсутствие виртуальных веток (branches в git). Надо сказать, в сообществе разработчиков до сих пор нет единого мнения, нужны ли они в этой системе вообще. Для введения новых экспериментальных изменений, для которых чаще всего и используются ветки, Darcs создаёт клон репозитория в отдельной директории в файловой системе. После тестирования, этот клон сливается с тем репозиторием, который считается стабильным или готовым к релизу, после чего удаляется, либо остаётся для истории, если идея не пошла дальше эксперимента. Таким образом, ветки не заполняют собой индекс основного репозитория, оставляя его максимально чистым. А обзор «веток» происходит в простом файловом менеджере.
> Better handling of binary files Darcs can handle binary files but saves each version of a binary file as a complete file, whereas Subversion only stores the diff between versions of a binary. See http://bugs.darcs.net/issue1233 .

если миллиард сообщений в группе, то коммит каждого нового точно не может быть на весь список - это слишком затратно
значит, бьем директорию/индекс на страницы, допустим дерево побайтно, тогда 8192 займет страница на 256 адресов по 32 байта
tree у tree? как-то не звучит, надо назвать каталогом что ли может?

> catalog – определения
> Существительное
> 1. a complete list of items, typically one in alphabetical or other systematic order.

вполне годится для обобщения, что не только файлы (про folder сказать, что тоже вызовет путаницу, несмотря на историю)

если каталог сообщений ньюсгруппы, не слишком ли короткий тогда GUID? он ведь для мутабельных объектов
а сообщения иммутабельны, тут редактирование делается control messages, в fossil - P card (parent, как у коммита)

> In theory, it is sufficient for follow-up posts to have only an I card, since the G card value could be computed by following a chain of I cards. However, the G card is required in order to associate the artifact with a forum thread in the case where an intermediate artifact in the I card chain is shunned or otherwise becomes unreadable. 
> The optional P card specifies a prior forum post for which this forum post is an edit. For display purposes, only the child post is shown, though the historical post is retained as a record. If P cards are used and there exist multiple versions of the same forum post, then I cards for other artifacts refer to whichever version of the post was current at the time the reply was made, but G cards refer to the initial, unedited root post for the thread. Thus, following the chain of I cards back to the root of the thread may land on a different post than the one given in the G card. However, following the chain of I cards back to the thread root, then following P cards back to the initial version of the thread root must give the same artifact as is provided by the G card, otherwise the artifact containing the G card is considered invalid and should be ignored.
> In general, P cards may contain multiple arguments, indicating a merge. But since forum posts cannot be merged, the P card of a forum post may only contain a single argument. 

- вот по сути похоже на проблему "промежуточные коммиты недоступны"


https://fossil-scm.org/home/doc/trunk/www/sync.wiki :
> 1.1 Conflict-Free Replicated Datatypes
> The "bag of artifacts" data model used by Fossil is apparently an implementation of a particular Conflict-Free Replicated Datatype (CRDT) called a "G-Set" or "Grow-only Set". The academic literature on CRDTs only began to appear in about 2011, and Fossil predates that research by at least 4 years. But it is nice to know that theorists have now proven that the underlying data model of Fossil can provide strongly-consistent replicas using only peer-to-peer communication and without any kind of central authority.
> If you are already familiar with CRDTs and were wondering if Fossil used them, the answer is "yes". We just don't call them by that name. 

===
03.04.24
как эмулировать чаты, когда у форумов есть только треды, для поддержки моста Telegram? видимо, как ответы на "корневое" сообщение?
- тогда у него будет число реплаев в индексе расти? сделать индекс
  специального типа?
- порядок сообщений? может, права записи только у моста / сабмит как в NNTP на
  премодерацию? а что, заявить это intentionally slow to motivate for threads
  - сделать разделение на ordered / unordered chats и объяснить это в FAQ?

HDF5:

Symbol Table - напоминает атомы

> Object Reference Count
> This value specifies the number of “hard links” to this object within the current file. References to the object from external files, “soft links” in this file and object references in this file are not tracked.

-- подумать насчет holding/embedded relationships

> Creation Order
> This 64-bit value is an index of the link’s creation time within the group. Values start at 0 when the group is created an increment by one for each link added to the group. Removing a link from a group does not change existing links’ creation order field.

-- может пригодиться?

> Link information
>
> The format of this field depends on the link type.
>
> For hard links, the field is formatted as follows:
> Size of Offsets bytes: 	The address of the object header for the object that the link points to.
>
> For soft links, the field is formatted as follows:
> Bytes 1-2: 	Length of soft link value.
> Length of soft link value bytes: 	A non-NULL-terminated string storing the value of the soft link.
>
> For external links, the field is formatted as follows:
> Bytes 1-2: 	Length of external link value.
> Length of external link value bytes: 	The first byte contains the version number in the upper 4 bits and flags in the lower 4 bits for the external link. Both version and flags are defined to be zero in this document. The remaining bytes consist of two NULL-terminated strings, with no padding between them. The first string is the name of the HDF5 file containing the object linked to and the second string is the full path to the object linked to, within the HDF5 file’s group hierarchy.
>
> For user-defined links, the field is formatted as follows:
> Bytes 1-2: 	Length of user-defined data.
> Length of user-defined link value bytes:
> The data supplied for the user-defined link type.

-- надо ли дополнительные типы линков?

как-то там Fractal Heap и Extensible Array плохо описаны (последний вообще похож на подходящий для MFT или NNTP overview), надо перечитать/PrivateHeap.pdf
хотя prefix tree для append only может и заменить наверное
===
04.04.24
нашел в https://github.com/jamesmudd/jhdf/blob/920a3f14ccd8177ab881fd19f5f0477ceec60b2f/jhdf/src/main/java/io/jhdf/dataset/chunked/indexing/ExtensibleArrayIndex.java#L237
ссылку на https://bitbucket.hdfgroup.org/projects/HDFFV/repos/hdf5doc/browse/projects/1_10_alpha/ReviseChunks/skip_lists
и там https://bitbucket.hdfgroup.org/pages/HDFFV/hdf5doc/master/browse/projects/1_10_alpha/ReviseChunks/skip_lists/SkipListChunkIndex.html
объясняет, как сначала реверснули скип-листы от tail'а в начало, а потом пришли к массиву

...похоже, оно опирается на то, что размеры блоков можно удваивать сколько угодно
да и оверхед у не-data блоков, больше степени двойки выходит, по эксельке

блоки разного размера, например верхние индекс-блоки, можно же в специальном файле держать, а-ля как XOR-данные

попробовать Pyramint?
===
05.04.24
если берём в 4 байта Pyramint5 и в старших 27 битах номер CG, тогда Pyramint5 указывает на размер и адрес блока внутри cg
т.е. 16 блоков по 4K, 8 блоков по 64K ... 1 блок по 64 Мб, всего 128 Мб отсек, и 2^5 = 32 описателя свободного места - чья запись MFT, пусть её адрес тоже укладывается в 4 байта
ну пусть для Pyramint6 будет 64 адресов на CG, это 256 байт потребного MFT всего
нулевой блок всегда под мету, на MFT тоже можно ссылаться как номер CG + номер внутри, а делить при желании нечётно, или выделить direct/indirect-блоки

или вообще зачем делить? внутри CBOR вида
  base => { ... },
  direct_block => {
    for_1234 => [],
  },
  indirect => ...

хотя нет, ссылки же нужны, значит массив записей, внутри уже тип
если запись undef, она никогда не была аллоцирована, если тип пустая строка, это FREE, и
после base txn освобождения и может быть 0+ filler - строки с предыдущим
содержимым для дебага / ограничения роста других записей

до CBOR его еще надо описать
struct cg {
	uint64_t cg_magic;		/* magic number "PBFS\r\n\x1a\n" */
	uint32_t cg_cgx;		/* we are the cgx'th cylinder group */
	uint32_t cs_nfree;		/* total free space as number of 512-byte units */
	uint16_t cg_cborsz;		/* size of CBOR data */
	uint8_t  cg_cboroff;		/* offset to CBOR in 32-bit words from beginning */
	uint8_t  cg_sizes;		/* 4:1:3 |adjustP|blkshift| zero add meta blks or last minus blks, Pyramint flavour, log2(block size)-9 */
	uint32_t cg_cksumhash;		/* checksum hash of entire block */
	uint64_t cg_time_txn;		/* time last written & transaction number */
	int32_t  cg_sparecon32[2];      /* reserved for future use */
	uint8_t cg_space[1];		/* space for free map array & CBOR */
/* actually longer */
};

tag256 => в SUPER для компрессии, прописать другие теги (66 и 70 - массив uint32)
dinode
#define	UFS_NXADDR	2		/* External addresses in inode. */
#define	UFS_NDADDR	12		/* Direct addresses in inode. */
#define	UFS_NIADDR	3		/* Indirect addresses in inode. */
sys/ufs/ffs/ffs_inode.c:#define SINGLE  0       /* index of single indirect block */
sys/ufs/ffs/ffs_inode.c:#define DOUBLE  1       /* index of double indirect block */
sys/ufs/ffs/ffs_inode.c:#define TRIPLE  2       /* index of triple indirect block */

начнём считать максимальное число блоков, пусть Pyramint5 и 4096 блоки, тогда:
* 2^27 cg * 16 * 4096 = 2^43, еще недостаточно для 2^48
* добавляем 2^27 * 8 * (16*4096) = 2^46 тоже мало
* и 2^27 * 4 * (256*4096) = 2^49 уже перебор, т.е. хватит и половины
  мегабайтных блоков
= потрачено: 16 блоков по 4K + 8 по 256K + 2 по 1M = 26 блоков
=> их указатели займут 2^27 * 26 * 4 байт
оно и так превышает 2^48, но пусть докинем еще 1/128 оверхеда на номер
страницы в -journal для min размера страницы,
итого 12800 + 128*5 = 13440 MiB max expected block pointers in MFT
из всего максимум доступных 2^27 * ~4096 = 512 Gb MFT - ~400 файлов макс. размера
при общем размере FS 2^27 * 2^27 = 16 Pb

у L и S разный формат: в L сразу адреса, всегда 4 байта, в S надо пару: длина/оффсет и ссылку на номер в MFT

а вообще пусть не S, а I, как у дерева, хотя и списки, просто того же типа на любом уровне выше L
а париться с первыми записями прямо из - не нужно, т.к. не фиксированный размер позволяет просто создать рядом маленький direct или indirect да и всё
  D => { first => 1234, last => 6789, second_ofs => 0x123400 },
  I => [
    { first => 234, last => 567, second_ofs => 0x123400 },
    { first => 345, last => 678, second_ofs => 0x234500 },
  ],
===
06.04.24
following HDF5 signature, make it invalid for text files - UTF-8 and UTF-16 in
both endianness and alignments: "\xD8\D8\xFE\xFE\x01\xD8\D8\xFE\xFEPMFT", where \01 is major version 1
всё-таки сделать иноды и каталоги? fuse хочет, да и если /boot на него?
тогда посмотреть на его объемы - ls -R почти 2000 файлов, это в 4 Кб не влезет
значит связный список, DIRF для первого и DIRE для дальше, внутри массив чтобы смещение для getdents можно было проще в отличие от мапа
d_type можно закодировать в верхние 4 бита CG number, всё равно много файлов не требуется:
#define	DT_UNKNOWN	 0
#define	DT_FIFO		 1
#define	DT_CHR		 2
#define	DT_DIR		 4
#define	DT_BLK		 6
#define	DT_REG		 8
#define	DT_LNK		10
#define	DT_SOCK		12
#define	DT_WHT		14

/*
 * Convert between stat structure types and directory types.
 */
#define	IFTODT(mode)	(((mode) & 0170000) >> 12)
#define	DTTOIF(dirtype)	((dirtype) << 12)
директории же нельзя shrink на ходу из-за оффсета в getdirentries... карта свободных? список?
если вернуть имена в саму иноду, то директория просто массив номеров инод, без пар, мало места
если pbmfs монтируется в отдельный раздел, то блочный файл (raw device) занят
её драйвером - тогда все gaps между cg отвести в специальный файл без указателей (любой оффсет рассчитывается по инфе)
это для fuse, для перформанса там может быть geom вместо такого файла через драйвер, например
$MFT 0
$MFTMirr 1
$LogFile 2
$Volume 3
$AttrDef 4
. (Root Directory) 5
$Bitmap 6
$Boot 7
$BadClus 8
$Secure 9
$UpCase 10
$Extend 11
===
07.04.24
нет, наверное свободную запись всё же просто [] и filler только для RESERVED,
иначе свободное место просто по cg_cborsz посчитать будет сложно, предыдущее
содержимое можно в хвост записи, или может лучше в rollback journal?..
формат журнала: 4 Кб заголовок/индекс, потом текущий XOR MFT, потом прошлый XOR MFT, потом страницы журнала, сколько хватит индекса,
потом XOR-сектора каждого защищаемого файла и по XOR-группе секторов для этого файла на каждый отсек
по аналогии с https://www.sqlite.org/fileformat2.html#the_rollback_journal нужно нонс, чексумму каждой страницы
кольцевой буфер? тогда еще txn для каждой страницы указать надо... и нонс для каждого txn?
надо подогнать так, чтоб выделить ровное число сразу, 2 или 4 Мб, а остаток заголовка - MFT-номера защищаемых файлов
а зачем чексумму для каждой страницы отдельно? она же у нас есть в формате меты, в отличие от sqlite,
тогда индекс нужен только по txn каждой страницы (чтобы не читать их) и поля current txn и last ok tsn?
использовать в качестве txn полные 64 бита, включая таймштамп?
головная боль подогнать размер, чтоб минус заголовок и сумма ровная была, сколько ж тогда на recovery файлы останется...
чтобы не возиться с recovery пофайлово на каждый отсек, может просто сделать parity compartments?
младшие 5 бит номера - как индикатор/указатель расположения parity cg?
почитал код graid3, там логический сектор состоит из секторов всех data-дисков, поэтому и 2^n+1 дисков - это совсем другой алгоритм
а еще же надо как-то детектировать stale XOR... чексуммы для блоков тогда?
у ZFS вариации Fletcher2 и Fletcher4 совсем не как в https://en.wikipedia.org/wiki/Fletcher's_checksum - а 32 байта без модулей
в 4 Кб и двухбайтные-то влезут лишь при очень большой полосе, не вариант
следующая идея: уполовинить размер parity CG, ведь XOR же пофиг сколько их - т.е. для 128 Мб cg, в 64 Mb-блоке лежат XOR, для половин каждого data cg
тогда для каждого 4 Кб XOR-сектора можно парный ему сектор - с чексуммами и таймштампами его и всех data-секторов всех защищаемых CG
в первых 4 Кб соответственно для меты тоже, чем она хуже... но нужен ли битмап инициализированных блоков?
для ровного можно 64 байта сделать: таймштамп, номер cgx + его половина (1 бит хватит?), id алгоритма, reserved и остаток хватит и для SHA-384
тогда compartment и cylinder group таки разные понятия - отсек есть несколько cg возле одной с parity
нет, не 1 бит, а полные адреса блоков для чексумм - мы же, возможно, храним скажем через 3 отсека, дать возможность выбора алгоритмов
может кстати для gap между cg, то есть $Volume файла, тоже дать эту защиту... сколько бит адреса дать?
тогда же можно и growfs сделать на несколько дисков, с одновременным как бы raidz1 - fs превращается в zpool...
или нет, если вынуть один диск, его gaps потеряются...
или да, но он должен целиком состоять из parity cg без gaps... тогда всё становится не просто с last cg
нет, для вынимания диска нужно XOR в его полном объеме, а тут только половина cg
===
09.04.24
с прямыми ссылками на диск понятно, а вот на другие записи меты... 27 бит + 5 бит + 44 бит - неровно и много
с индексными блоками наверху некрасиво - длину скомпрессить бы, а то 2^32 секторов не хватит на полную базу
и если пусть 8 байт/килобайт, 7 бит на уровень - много, это 7 disk accesses для полного файла?..
плюс поиск свободных номеров меты, да и куски меты по 4 - бить неудобно для тех же каталогов...
возникла идея - а что, если оставить адрес по 27 бит + 5 номер/размер внутри, но вынести мету отдельно?
тогда и номера блоков будут использованы полностью все 32, без возни с размером начального сектора для MFT
так как оно большое, объединить сразу с XOR-блоками в верхней половине?
тогда при 1 записи в 4 Кб, возвращаемся к старым цифрам: из 32768 бит вычитаем 64 байта, получается 126 Мб MFT
2 Мб пускаем на Journal, на карты владения - 2^20 по 128 байт на цилиндр на Pyramint5 = 8192 cylinders per CG
еще надо несколько блоков на summary по free и на compartments - XOR-цилиндры каждые N цилиндров
где делать gaps? между compartments?
наверное да, и начинать compartment с XOR-цилиндра, т.к. в нём есть сигнатура (для поиска между gaps при починке)
начал читать про XFS: https://mirrors.edge.kernel.org/pub/linux/utils/fs/xfs/docs/xfs_filesystem_structure.pdf
The di_next_unlinked value in the inode is used to track inodes that have been unlinked (deleted) but are still
open by a program. When an inode is in this state, the inode is added to one of the AGI’s agi_unlinked hash
buckets. The AGI unlinked bucket points to an inode and the di_next_unlinked value points to the next inode
in the chain. The last inode in the chain has di_next_unlinked set to NULL (-1).
Once the last reference is released, the inode is removed from the unlinked hash chain and di_next_unlinked
is set to NULL. In the case of a system crash, XFS recovery will complete the unlink process for any inodes found in
these lists.
The only time the unlinked fields can be seen to be used on disk is either on an active filesystem or a crashed system.
A cleanly unmounted or recovered filesystem will not have any inodes in these unlink hash chains.

-- вот в специальный файл их засунуть...

rc_startblock
AG block number of this record. The high bit is set for all records referring to an extent that is being used to
stage a copy on write operation. This reduces recovery time during mount operations. The reference count of
these staging events must only be 1.

-- а вот это уже для основной sqlite fs

di_projid
Specifies the owner’s project ID in v2 inodes. An inode is converted to v2 if the project ID is set. This value
must be zero for v1 inodes.
di_projid_hi
Specifies the high 16 bits of the owner’s project ID in v2 inodes, if the XFS_SB_VERSION2_PROJID32BIT
feature is set; and zero otherwise.

-- у них это для квоты, но может для репы тоже сгодится...

di_changecount
Counts the number of changes made to the attributes in this inode.
===
10.04.24
typedef uint32_t pbmft_no_t;
typedef uint32_t pbmfs_daddr_t;
typedef uint64_t pbmft_extentr_t; /* 35 bit sum (S_BLKSIZE), 29 bit MFT no */

/* Four 6-byte inode times:                                      */
/*	mft_time_t	di_atime;	 Last access time.       */
/*	mft_time_t	di_mtime;	 Last modified time.     */
/*	mft_time_t	di_ctime;	 Last inode change time. */
/*	mft_time_t	di_birthtime;	 Inode creation time.    */
typedef uint8_t  pbmfs_itimes_t[24];

struct mft_record_head {
	uint32_t magic;
	uint16_t flagsizes;	/* 2:generation, 2:FixedSz, 12:VarSz, both 0 in free */
	uint16_t txn;		/* in which transaction written */
	uint32_t checksum;	/* entire page, including free space */
} /* 12 bytes */

struct mft_inode_common {
	uint16_t	di_mode;	/*  12: IFMT, permissions; see below. */
	uint16_t	di_freelink;	/*  14: Next unlinked inode in this CG. */
	uint32_t	di_uid;		/*  16: File owner. */
	uint32_t	di_gid;		/*  20: File group. */
	uint32_t	di_flags;	/*  24: Status flags (chflags). */
	pbmfs_itimes_t	di_times;	/*  28: Four 6-byte times */ 
	pbmft_no_t	di_extattr;	/*  52: Reserved for future use */
}

struct mft_file {
	struct mft_record_head	mft_head; /* "FILE" */
	struct mft_inode_common inode_common;
	uint64_t	di_size;	/* 56: File byte count. */
	uint64_t	di_blocks;	/* 64: Blocks actually allocated, S_BLKSIZE units */
	pbmfs_daddr_t	di_parity_blk;	/* Overall parity block, if present. */
	uint32_t	di_spare[5];	/* Reserved; currently unused */
	char		di_cbor[4000];	/* Variable attributes part */
}

/* Because 640K of SQLite databases ought to be enough for everyone! */
#define MFT_NLDIRECS	645		/* 640*1024/((4096-32)/4) */
#define MFT_NHDIRECS	40		/* 640*1024-655320 */
struct mft_dirbase {
	struct mft_record_head	mft_head; /* "DIRB" */
	struct mft_inode_common inode_common;
	uint64_t	dirb_counters;	/* Bit fields:
		20 bits	dirb_totalfree;  Total free entries in directory 
		20 bits	dirb_totalalloc; Total allocated entries in directory 
		6 bits 	dirb_freecount;	 Number of free entries in this record 
		6 bits	dirb_thisalloc;	 Integrity checking: entries this record
		10 bits	dirb_nrecords;	 Total number of MFT records for this directory */
	int16_t		dirb_nchildir;	/* To fake di_nlink of ".."'s */
	uint16_t	dirb_leaffree[MFT_NLDIRECS]; /* N free entries in each leaf */
	pbmft_no_t	dirb_dentries[MFT_NHDIRECS]; /* entries directly in head */
	pbmft_no_t	dirb_leaves[MFT_NLDIRECS]; /* MFT record numbers of leaves */
}

struct mft_dirl {
	struct mft_record_head	mft_head; /* "DIRL" /*
	pbmft_no_t	dirb_base;	/* 12: MFT directory base record */
	pbmft_no_t	dirb_next;	/* 16: MFT number of next record in list */
	pbmft_no_t	dirb_prev;	/* 20: MFT no of previous record in list */
	uint16_t	dirb_freecount;	/* 24: Number of free entries in this record */
	uint16_t	dirb_thisalloc;	/* 26: Integrity checking: entries this record */
	uint16_t	dirb_spare[2];	/* 28: Reserved; currently unused */
	pbmft_no_t	dirb_dentries[1016]; /* entries of this record */
}

struct mft_addrl {
	struct mft_record_head	mft_head; /* "ADRL" /*
	uint32_t	adrl_spare1;    /* 12: Reserved for alignment */
	uint64_t	adrl_offset;	/* 16: Logical offset in file */
	uint64_t	adrl_size;	/* 24: Sum size of all entries in this record */
	pbmft_no_t	adrl_base;	/* 32: MFT inode base record */
	pbmft_no_t	adrl_next;	/* 36: MFT number of next record in list */
	pbmft_no_t	adrl_prev;	/* 40: MFT no of previous record in list */
	pbmft_no_t	adrl_parent;	/* 44: MFT no of parent (I) node */
	uint32_t	adrl_spare[4];  /* 48: Reserved; currently unused */
	pbmfs_daddr_t	adrl_entries[1008]; /* entries of this record */
}

struct mft_addri {
	struct mft_record_head	mft_head; /* "ADRI" /*
	uint32_t	adri_spare1;    /* 12: Reserved for alignment */
	uint64_t	adri_offset;	/* 16: Logical offset in file */
	uint64_t	adri_size;	/* 24: Sum size of all entries in this record */
	pbmft_no_t	adri_base;	/* 32: MFT inode base record */
	pbmft_no_t	adri_next;	/* 36: MFT number of next record in list */
	pbmft_no_t	adri_prev;	/* 40: MFT no of previous record in list */
	pbmft_no_t	adri_parent;	/* 44: MFT no of parent (I) node */
	uint32_t	adri_spare[4];  /* 48: Reserved; currently unused */
	pbmft_extentr_t	adri_entries[504]; /* entries of this record */
}

...а checksum-то забыл! придется di_gen перенести в 2 бита в заголовке
и di_freelink сделать 2-байтным - только на одну CG...

Идея для уровня повыше: slab-аллокатор как UMA в ядре - для attribute tails
> For the following computations, let U be the usable size of a database page, the total page size less the reserved space at the end of each page. And let P be the payload size. In the following, symbol X represents the maximum amount of payload that can be stored directly on the b-tree page without spilling onto an overflow page and symbol M represents the minimum amount of payload that must be stored on the btree page before spilling is allowed. 
> M is always ((U-12)*32/255)-23.

u=4096
((u-12)*32/255)-23
489.50196078431372549019
u=1024
((u-12)*32/255)-23
103.99607843137254901960

- похоже не требуется, достаточно отдельную sqlite-базу со страницей побольше...
хотя не совсем, их же размер может скакать при ребалансировке
-> просто сделать пачку таблиц типа bucket_400, bucket_420 ? тогда надо
   сквозной id, отдельная таблица с референсом на владельца, а в этих просто
   (rowid, blob)

volmap => [
    {
	offset => 0x0,
	length => 65536,
	type   => GAP,
    },
    {
	offset => 65536,
	length => 128*1048576,
	type   => MFT,
    },
],
===
11.04.24
если rollback journal делать по принципу sqlite, мы же не контролируем данные
файлов, не будет ли рассогласования, если откат транзакции меты, а данные
файлов изменились?

> SQLite assumes that when data is appended to a file (specifically to the rollback journal) that the size of the file is increased first and that the content is written second. So if power is lost after the file size is increased but before the content is written, the file is left containing invalid "garbage" data

-- so for MFT to be able to use same rollback journal technique, the write() of data
after a seek() extending a file must not return until MFT transaction is committed

отнимать от Pyramint5 имеет смысл только последние 4 блока максимум?
оно неровное, 11011 значит 1024, но находится на уровне 256, т.е. это 1280 блоков
а следующий 11100 для 2048, это непокрытое место что ли?
похоже, длины экстентов будут неровными по уровням - считать сам Pyramint как смещение, а  длину нужно вычитать из следующего смещения,
появляются дополнительные размеры - 1 по 128, 1 по 1024, а по 16 всего 7, и по 256 всего 3
т.е. массив счетчиков free summary { 1, 16, 128, 256, 1024, 2048, 4096, 8192, 16384 } на 9 элементов
===
13.04.24
карта volmap как набор uint64_t, первый всегда смещение, 62 бита, в верхних type:
00 - следующий просто длина gap
01 - два слова uint32_t: 

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |Blksh-9|P|Flags|DecremB|Typ|CompartSz|      Repeat Count       |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |            Following Upper-Layer Gap Length                   |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

---
array of uint32_t's as micro-CBOR: on top level, offset word(s):

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |S|L|                      Offset Low part                      |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |              Offset High part, present if L is set            |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

if S is 0, then this word is offset (mnem 0 for O), else Structured

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |1|Major|                                                       \
     +-+-+-+-+      Structured type, one or more words               /
     \                                                               \
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

for Major type 0, it is just Gap length:

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |1|  0  |          Following Upper-Layer Gap Length             |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

Major types 2 to 7 is array of Major-1 elements (like CBOR), with the
following additional information for compression:

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |1| 2-7 |     Repeat Count      | Shift |    UL Gap Length      |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

if Repeat Count > 0, then additional Gap field, if non-zero is inserted
between repetitions - intra-elements, like `join(gap, body)` function.

Shift means actual_gap = gap_length << shift, so values up to 0xfff80 can be
encoded (in 512-sectors, 512 Mb minus 64 Kb)

And most important, Major type 1 describes PBMFS region:

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |1 0 0 1|Blksh-9|P|Flags|DecremB|Typ|CompartSz|
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

не хватает ни Repeat Count нормально, ни Gap length внутри
---
вариант с uint16_t:

move constants Blshift and Pyramint outside; start with two bits
on top level, for first in pair it is size of ofsset:

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |Num|                                                           \
     +-+-+         Offset, Num + 1 words (16/32/48/64 bits)          /
     \                                                               \
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

for second in top-level pair, or inside it's structure, it's scalar or array:

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |0 0|T|L|    Data of type T, 16 bits or 32 bits if L is set     |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

      0                   1
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |Arr|J|       Repeat Count      |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

if Arr>0, it's array with Arr+J elements, and for array if Repeat Count > 0,
then J means `join(gap, body)` and first element MUST be Gap Length scalar:

      0                   1                   2                   3
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |0 0|0|L|Shift|        Following Upper-Layer Gap Length         |
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

And most important, type 1 describes PBMFS region:

      0                   1
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |0 0|1 0|S|T/Flg|DecrB|CompartSz|
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

S - soft-reserved flag

0 - usual compartment group
1 - usual compartment group with XOR cylinder
2 - MFT compartment group
3 - MFT compartment group with XOR cylinder
4 - halved MFT compartment group with XOR cylinder
5-7 - last compartment group of partition:

      0                   1
      0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
     |0 0|1 0|S|1|DecrB+4|X|CompartSz|
     +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

  where X is XOR cylinder flag.

That is, usual cylinder can be at most 7 blocks less than full Pyramint (0b11000,
in Pyramint5 is 256 for 512 total - 2 Mb on 4K), and last CG can be 11 blocks
less (0b10100, in Pyramint5 is 80 for 96 total - 384 Kb on 4 K block)
===
15.04.24
NOTE that on top-level, pair - offset and scalar/structure - form something
like a "partition" in partition tables. That is, in GPT partitions do not need
to be strictly consequently on disk:

#    Offset       Size    No. Name (Size)
=>       40  250069600  ada3  GPT  (119G)
         40   33654432     3  swapssd  (16G)
   33654472  203423744     1  (null)  (97G)
  237078216   12991424        - free -  (6.2G)

and so are entries in Volume Map. This is for cases when volume has low free
space, and new entries for PBMFS are allocated in whatever free regions of
lesser size - they will be in order of increasing cylinder number, but offset
may be less than of previous entries. But in contrast to GPT, sum of entries
MUST cover the volume contiguously, without "free" space (there are gaps for
it). And that's why the format is not generic CBOR but somewhat crunched:
space for such fragmentation is limited.

посмотрел, на UFS atime может быть больше mtime/ctime (а вот те 2 всегда равны),
но для слепого журналирования надо же всю страницу копировать
если вынести atime из иноды, то две выгоды:
1) уменьшается размер inode_common, снова есть reserved
2) MFT-записи atime можно не журналировать

тогда в одном секторе указываем старшее слово, для каких MFT он, и записи по
8 байт - младшее слово pbmft_no_t и 6 байт собственно atime:
struct mft_atime {
	struct mft_record_head	mft_head; /* "ATIM" */
	uint16_t	atim_mftno_hi;	/* 12: High 16 bits of pbmft_no_t of entries in this record */
	uint16_t	atim_spare[9];	/* 14: Reserved; currently unused */
	uint64_t	atim_entries[508]; /* entries of this record */

};
struct mft_inode_common {
	uint16_t	di_mode;	/*  12: IFMT, permissions; see below. */
	uint16_t	di_spare16;	/*  14: Reserved for future use. */
	uint32_t	di_uid;		/*  16: File owner. */
	uint32_t	di_gid;		/*  20: File group. */
	uint32_t	di_flags;	/*  24: Status flags (chflags). */
	pbmfs_itimes_t	di_times;	/*  28: Three 6-byte times */
	uint16_t	di_atime_no;	/*  46: Record with atime of this inode. */
	pbmft_no_t	di_freelink;	/*  48: Next unlinked inode. */
	pbmft_no_t	di_extattr;	/*  52: Reserved for future use */
}

к 03.04.24 про Creation order действительно идея может пригодиться?
да, для cookie в getdents/getdirentries при реализации в btree например - что вполне позволит shrink спереди при удалении самых старых элементов
- надо проверить по /usr/src, что оффсеты не с нуля
- в sys/fs/tmpfs/tmpfs_subr.c похоже оно, там RB-дерево
===
20.04.24
в обычной иноде есть её имя в CBOR, а в каталоге-то нет!
не 10 байт же отводить, даже если common_inode сократить...
попробуем по принципу из ReiserFS - начало имени + его хэш, чтоб лукап каждой FILE был только при коллизиях

#define MFT_NLDIRECS	645		/* 640*1024/((4096-32)/4) */
#define MFT_NHDIRECS	40		/* 640*1024-655320 */
struct pbmfs_dentry_t {
	pbmft_no_t	dire_fileno;	/* MFT rec of inode */
	uint8_t		dire_dtype;	/* d_type << 4 | has hash (name truncated) */
	union {
		char	dire_fname[11]; /* full name fits, no hash needed */
		struct {
		    char dire_tname[7];	/* truncated name */
		    uint32_t dire_hash;
		} nh;

	} ud;
};

struct mft_dirbase {
	struct mft_record_head	mft_head; /* "DIRB" */
	struct mft_inode_common inode_common;
	uint32_t	dirb_totalfree;  /* 56: Total free entries in directory */
	uint32_t	dirb_totalalloc; /* 60: Total allocated entries in directory */
	uint16_t	dirb_nrecords;	 /* 64: Total number of MFT records for this directory */
	int16_t		dirb_nchildir;	 /* 66: To fake di_nlink of ".."'s */
	uint8_t		dirb_freecount;	 /* 68: Number of free entries in this record */ 
	uint8_t		dirb_thisalloc;	 /* 69: Integrity checking: entries this record */
	uint16_t	dirb_spare[13];	 /* 70: Reserved; currently unused */
	char		dirb_name[128];  /* 96: Name of us in parent directory */
	pbmfs_dentry_t	dirb_dentries[MFT_NHDIRECS]; /* 224: entries directly in head */
	uint8_t 	dirb_leaves[MFT_NLDIRECS*5]; /* MFT record numbers of leaves:
	         pbmft_no_t of leaf and N free entries in 4 leaves ~divided by 4 */
};

struct mft_dirl {
	struct mft_record_head	mft_head; /* "DIRL" /*
	pbmft_no_t	dirb_base;	/* 12: MFT directory base record */
	pbmft_no_t	dirb_next;	/* 16: MFT number of next record in list */
	pbmft_no_t	dirb_prev;	/* 20: MFT no of previous record in list */
	uint8_t		dirb_freecount;	/* 24: Number of free entries in this record */
	uint8_t		dirb_thisalloc;	/* 25: Integrity checking: entries this record */
	uint16_t	dirb_recno;	/* 26: We are MFT no N - to calc d_off */
	uint16_t	dirb_collisflg;	/* 28: Collisions: from us? before us? */
	uint16_t	dirb_spare;	/* 30: Reserved; currently unused */
	pbmfs_dentry_t	dirb_dentries[254]; /* entries of this record */
};
===
23.04.24
#*** файл с несколькими атрибутами данными концептуально подобен составному
письму в MIME, см. структурные запросы в IMAP4 - то есть, у каждого,
получается, могут быть свои MIME-типы и др. заголовки? пересмотреть термин
атрибута тогда?
Message-Id: надо же поддержать совместимость, а у нас хэш
- к записям от 29/31.03.24 добавить дополнительный формат: подстроку Id, флаг внешней конвертации

а какая длина у Message-Id может быть?

msg-id          =       [CFWS] "<" id-left "@" id-right ">" [CFWS]
id-left         =       dot-atom-text / no-fold-quote / obs-id-left 
id-right        =       dot-atom-text / no-fold-literal / obs-id-right
no-fold-quote   =       DQUOTE *(qtext / quoted-pair) DQUOTE
   
Resnick                     Standards Track                    [Page 23]
^L 
RFC 2822                Internet Message Format               April 2001

no-fold-literal =       "[" *(dtext / quoted-pair) "]"

dtext           =       NO-WS-CTL /     ; Non white space controls
                        %d33-90 /       ; The rest of the US-ASCII
                        %d94-126        ;  characters not including "[",
                                        ;  "]", or "\"

qtext           =       NO-WS-CTL /     ; Non white space controls
                        %d33 /          ; The rest of the US-ASCII
                        %d35-91 /       ;  characters not including "\"
                        %d93-126        ;  or the quote character

atext           =       ALPHA / DIGIT / ; Any character except controls,
                        "!" / "#" /     ;  SP, and specials.
                        "$" / "%" /     ;  Used for atoms
                        "&" / "'" /
                        "*" / "+" /
                        "-" / "/" /
                        "=" / "?" /
                        "^" / "_" /
                        "`" / "{" /
                        "|" / "}" /
                        "~"

atom            =       [CFWS] 1*atext [CFWS]

dot-atom        =       [CFWS] dot-atom-text [CFWS]

dot-atom-text   =       1*atext *("." 1*atext)

quoted-pair     =       ("\" text) / obs-qp

text            =       %d1-9 /         ; Characters excluding CR and LF
                        %d11 /
                        %d12 /
                        %d14-127 /
                        obs-text

...итого, значит, фолдинг запрещен, то есть 998 лимит на строку, пусть первый CFWS 997, и без угловых = 995
===
26.04.24
продолжая запись от 20.04 - некрасиво как-то, а что там в ZFS?
а там чанки одинакового размера, и листовая хэш-таблица ссылается на них
кстати, это же примерно как если бы было в памяти - LIST_ENTRY по выровненному адресу
да, если адресоваться сразу на глобальный оффсет, а не блок MFT, то и размер каталога может расти постепенно по мере добавления, а не сразу аллоцировать под весь хэш
а как быть с 640 K ? :) 3-байтные элементы хэша?..
ну может ограничить реально 640 Кб на каталог, а не записей столько?
только с next-указателем, получается, надо больше 16 байт на запись.

/* Because 640K ought to be enough for everyone! */
#define MFT_NLDIRBLKS	(640/4)		/* 4 Kb one record / 4 byte pointer */

struct pbmfs_dentry_t {
	pbmft_no_t	dire_fileno;	/* MFT rec of inode */
	uint16_t	dire_hnext;	/* 1: has hash (name truncated) | 15: next in chain */
	union {
	    char	dire_fname[26]; /* full name fits, no hash needed */
	    struct {
	        char	dire_tname[22];/* truncated name */
	        uint32_t dire_hash;
	    } nh;

	} ud;
};

struct mft_dirbase {
	struct mft_record_head	mft_head; /* "DIRB" */
	struct mft_inode_common inode_common;
	uint16_t	dirb_totalfree;  /* 56: Total free entries in directory */
	uint16_t	dirb_totalalloc; /* 58: Total allocated entries in directory */
	uint16_t	dirb_firstfree;	 /* 60: First entry in free list */ 
	int16_t		dirb_nchildir;	 /* 62: To fake di_nlink of ".."'s */
	pbmft_no_t	dirb_parent;	 /* 64: To generate ".." record */
	uint8_t		dirb_nrecords;	 /* 68: Total number of MFT records for this directory */
	uint8_t		dirb_flags;	 /* 69: 0 = single base record, 1 = hash table */
	uint16_t	dirb_spare[13];	 /* 70: Reserved; currently unused */
	char		dirb_name[160];  /* 96: Name of us in parent directory */
	union {				 /* 256: */
	    char dirb_cbor_dentries[3840]; /* [ ["fullname",mftno,is_dir], ...] */
	    struct {
		pbmft_no_t dirb_leaves[MFT_NLDIRBLKS]; /* MFT record numbers of leaves */
		uint16_t dirb_hashtable[1597];	/* prime number */
		char	dirb_salt[6];		/* like zap_salt of ZFS */
		} hb;
	} ub;
};

struct mft_dirl {
	struct mft_record_head	mft_head; /* "DIRL" /*
	pbmft_no_t	dirl_base;	/* 12: MFT directory base record */
	pbmft_no_t	dirl_next;	/* 16: MFT number of next record in list */
	pbmft_no_t	dirl_prev;	/* 20: MFT no of previous record in list */
	uint8_t		dirl_recno;	/* 24: We are MFT no N - to calc d_off */
	uint8_t		dirl_freecount;	/* 25: Number of free entries in this record */
	uint8_t		dirl_thisalloc;	/* 26: Number of used entries in this record */
	uint8_t		dirl_firstfree;	/* 27: First free entry in this record */ 
	uint32_t	dirl_spare;	/* 28: Reserved; currently unused */
	pbmfs_dentry_t	dirl_dentries[127]; /* entries of this record */
};

Поначалу храним полные имена в CBOR, потом конвертируем в хэш, обрезая - с создаваемой в этот момент солью.
Чтобы проще считать размеры, инициализируем в 24 undef, конвертируем в хэш при
нехватке места или >255 записей, тогда длина внешнего массива всегда 2 байта.

альтернативная версия: если запись сделать 20 байт, то в 640 Кб влезут
204*160 = 32640 записей, вот почти полная утилизация бит, и dire_fname[14] как
в первом юниксе, но в mft_dirl так останется только лишь dirl_base
===
27.04.24
Если бы это была полноценная файловая система, то по аналогии с ZAP сделал бы name-value, но как slab allocator типа UMA с битмапом свободных и для имён хэш указывает на головы red-black tree: в каждой записи первые 3 uint64_t rbe_left, rb_right, rbe_parent со старшими 4 битами - сначала тип, free, red, black, value, direntry, ... в случае каталога в других 4 битах d_type и в последних 4 битах длина pad (пусть плитки отличаются по длине не более 16), дальше uint64_t иноды или адрес где value, и потом собственно имя. Выравнивание по cache line пожалуй не нужно (диск дороже), только по 8 или 16 байт. Чтобы уменьшить оверхед, можно самый старший бит - индикатор 32/64, тогда по байту или по 4 бита
===
28.04.24
к предыдущему дню: этот способ некуда применить в PBMFS, но он может оказаться полезным в самой сетевой файловой системе / контроля версий, см. запись от 02.04.24, ведь запрос только нужных страниц по сети подобен чтению только нужных блоков с диска - та же тема latency, только еще более
ну да, причем именно и только slab с битмапами: если у нас типа иммутабельные страницы по CoW, то есть при обновлении меняется Merkle tree наверх, то нельзя использовать first free / next free linked list - они могут стать invalid после обновления
slab имеет waste в хвостах, в muSCTP же надо передавать поменьше? ничего, по сети можно компрессию применить просто, а вот иметь унифицированный с диском формат - удобно
тогда и к записи от 23.04.24 - надо поддержать и длину 1024, и бинарные имена, а не NUL-terminated

r: 0 for black, 1 for red
L000 - free entry
L001 - value entry
L01r - RB TREE entry, tiny value - half size XXX something bad
L10r - RB TREE entry, tiny value - word size
L11r - RB TREE entry, value bytestring is in entry at offset

что здесь не так? вынести отдельно тип чтоб d_type был частный случай? тема NNTP overview?
почитал RFC 3977 про NNTP overview, там список полей в базе данных, это больше похоже на индексацию в SQLite в Fossil, чем на более стабильный формат его манифестов, так что такое уже в протокол вынести, видимо
а что делать с creation order заместо offset? linked list подходит плохо при удалениях, а счетчик позволит не менять имевшиеся записи (хотя при ребалансе дерева же меняются...)
нельзя уменьшать размер указателей - при балансировке же могут оказаться другие значения, более длинные, и не влезем в slab
читал исходники UFS, похоже там срать хотели на сохранение offset - там и directory compactification возможен, и перемещение записей внутри блока при фрагментации места свободных записей (кстати, не баг ли уж это в UFS?)
ну проблемы с "пока читаем, кто-то удалил или добавил" для DVCS нет, работа идёт на снапшоте
тогда можно было бы не беспокоиться насчет offset и сделать его prefix tree по creation order? ну пусть меняется offset при коммитах, а RB-tree уже на имена значения внутри, как и ранее
- редактирование сообщений можно было бы делать атрибутами values, но они
  удобны как 8 байт смещения, а если смещение меняется, то никак
- со счетчиком creation order как offset тогда не более 2^64 элементов за время жизни,
  мы же ограничены API где 64 бита off_t
===
29.04.24
а что, если старшие биты *basep в getdirentries()/lseek() заабьюзить? типа вот с таким флагом - мы итерируемся в порядке дерева, т.е. по алфавиту, с другим - по creation order, и т.д., и только без всего - по-старом как offset? Соответственно, сами "размеры" записей сделать малыми, хоть по 1 байту, собственно, как и tmpfs делает
можно даже по смещению 2^56 отдать записи, в которых d_fileno будет значению старшего байта, а d_name - поддерживаемой команде, такое capability query
- или нет, для каждой команды - по "все младшие нули" отдавать её тип/имя/пр.
  чтобы не тратить 256 смещений в каждом блоке
- ну или только две, где были бы "." и "..", и ограничить эти на 255 кстати,
  пока приложение еще не знает, какая максимальная длина
===
14.06.24
https://github.com/ScottArbeit/Grace?tab=readme-ov-file#commits-and-saves-checkpoints-and-promotions
идея про save/checkpoint/commit/promote - интересная, надо бы взять на вооружение
а всё остальное разочаровало: centralized, упор на облака, PaaS (Azure etc.), постоянный коннект к серверу - какой-то регресс

https://fossil-scm.org/forum/forumpost/013010c273932f90 в процессе написания ответа и далее думал - как же его скалабельнее сделать
15.06.24 00:03 а что, если каждый каталог - дочерняя репа, и каждый коммит создаёт "дерево" коммитов в каждом дочернем, по аналогии с tree objects в git?
тогда всяческие subtree/submodule прям integral part of design
правда, тогда проблема с концептом "каталог не только про файлы"?
если у каталога тоже есть UUID, тогда не проблема?
а если проскалировать вниз до каждой иноды, у каждой тоже есть UUID? каждый файл как RCS-репа
===
15.06.24
но если в каждом коммите повторять дерево "инода => коммит", чем это отличается от git subtree? еще и объемнее
причем для неизмененных поддеревьев получатся пустые коммиты зазря, waste - какой-нибудь web для CVS или SVN выводил только изменённые ревизии в этом каталоге же
а что, если по аналогии с монтированием файловых систем?
- типа в манифесте линейно два десятка записей и три каталога "инода => коммит", а остальное - листья, где собственные коммиты
- тогда это потребует заранее решать, где ниже по дереву делать коммиты, потом
  позже перевыделить отдельно либу с её историей не будет так просто

хорошо, пусть в духе CVS/SVN записываем по всей цепочке - только реально изменённые коммиты, но все одинаковые
- проблема с потерей места - куча одинаковых записей, потенциально больших
  description, PGP, sign, и что делать с merge parents?
- проблема с хэшами - дочерний в дереве не может сослаться на родительский,
  ведь тот еще не готов, пока не сделан родительский

тогда в каждом дочернем коммите записывать чисто UUID родителя в дереве, где находится коммит-родитель, если таковой есть?
- а если на либу-каталог ссылаются два разных проекта-родителя?
  - ну, не сможет проект-родитель ссылаться на HEAD, только на конкретную
    ревизию всегда, но вроде это и норм? TODO посмотреть, что могут git subtree/submodule в этом плане
- TODO продумать насчет насчет holding relationships и (avoid) циклов в дереве

собственно каталог объектов, теперь к каждому еще коммит приписать - делать это отдельным массивом сбоку? или расширять каталог до таблицы, типа вот не только имя и инода, но и коммит?
вопрос показа диффов - по идее, если список файлов в каталоге не менялся, то *этот* stream и не должен быть поменян, должен остаться тот же хэш
если каждому смещению в каталоге сопоставить id commit'а, то логически это просто CBOR-массив строк
- если бы они всегда были одинаковой длины (no extensibility), можно было бы
  просто бинарно, по смещению и всё, тогда delta encoding of diffs is simple
- но, видимо, надо тоже дерево - ведь для extensibility может быть разная
  длина, могут быть undef как пропуски... запрашивать только нужные страницы опять же
- получается два tree-спецформата: giant CBOR map & giant CBOR array?
- вообще тут перекликается тема с NNTP overview - для редактирования сообщения

TODO к записи от 02.04.24 и 27-29.04-24 - еще надо решить проблему "промежуточные коммиты недоступны"
что, если делать не slab, а просто страницы на фиксированное число записей по prefix tree? типа сколько получился размер, и ладно, и проблемы waste нет
и если всё-таки объединить, и назвать Pb table? Pb catalog? Petablob, 2^56 строк типа же
===
16.06.24
к записям от 29.03.24 и 31.03.24: а зачем минуты? и может расширить год?
10 бит год внутри тысячелетия, 4 месяц, 5 день, 5 час = 24 бита т.е. ровно 3 байта
это позволит первый байт полностью форматным, и разделить хэш бранча от типа, ведь типов предполагается мало
16.06.24 21:24 частичная упорядоченность по коммитам, а что если сдвинуть тип/бранч первым? для максимально возможной упорядоченности в типовом случае
тогда, если форматный байт сделать VarInt, пусть даже обычный как в SQLite или SDNV (RFC 6256), то:
* расширяемость сколько угодно
* можно вернуть хэш бранча прямо в этот VarInt?
  - тогда VarInt надо делать Little-Endian что ли?

можно формат опции CoAP: two nibbles for most common ones
а точно ли бранч лучше первым (в бинарном виде) для сортировки? что, если уже его хэш функция сменится?
Ver 2 бита, 10 бит год внутри тысячелетия, 4 месяц (отдельный ниббл в дампе), 5 день, 5 час - и 6 бит на формат
использовать для format VarInt по схеме из RFC 7541 Section 5.1 ?
===
17.06.24
надо понять, какая инфа вообще нужна в идентификаторе: пойти от того, как было бы в CBOR
сделать аргумент возможным для всех типов артифактов? 2 бита на длину: 0-3 байта

https://fossil-scm.org/forum/forumpost/71bf5bd5051a408e :
> (2) By Thomas Hess (luziferius) on 2024-03-21 15:42:39 in reply to 1 [source]
>
> I'd say the proper way is to import both into the *same* fossil repository. You can simply treat a git fork as a branch in fossil. (Github "forks" are basically fancy feature branches from the commit graph point of view.)
> So convert both git repositories into the export format, and then import them into the same fossil repository.
>
> (5.1) By Thomas Hess (luziferius) on 2024-03-21 17:20:52 edited from 5.0 in reply to 4
> Fossil doesn't like multiple open tips using the same branch name (switching branches by name becomes ambiguous), and importing both git repos created exactly this situation. So you should locate commits 3 or 4 using the timeline in the fossil UI,  
> hit the "Edit" button, and enter a new branch name under the option  
> "**Branching:** Starting from this check-in, rename the branch to:"
> You can choose to rename the source project "trunk/main/master" branch
>
> (6) By spindrift on 2024-03-21 17:32:35 in reply to 5.1 [link] [source]
>
> > Fossil doesn't like multiple open tips using the same branch name (switching branches by name becomes ambiguous)
>
> No, Fossil doesn't mind at all, and the branch selected is not ambiguous. It is simply the one with the newest leaf.
>
> (7) By Thomas Hess (luziferius) on 2024-03-21 17:58:30 in reply to 6 [link] [source]
>
> Well, yeah, It technically works without problems. But fossil complains, and you have to specifically allow forking via the `--allow-fork` switch, or fossil prevents you from creating a check-in that "would fork". (That's what I meant with "doesn't like")
> This is more or less a technicality and depends on the definition of the concepts.  
My interpretation is: In an asynchronous environment with multiple users, updating to "trunk" is ambiguous after a fork on trunk, and using the latest timestamp is a deterministic *resolution strategy* to resolve this. Nowhere is the timestamp inherent part of the branch name "trunk".
>
> If you have a project with multiple, active devs, and are currently on "trunk" with a fork, running `fossil update` is basically tossing a coin to determine on which concurrent timeline you end up. It can be the one you were on, or the other side of the fork. Without the fork, you know that after an update, the check-in you updated from at least stays an ancestor of the check-in you land on.
> 
> (9) By spindrift on 2024-03-21 19:45:39 in reply to 7 [link] [source]
>
> >  you have to specifically allow forking via the --allow-fork
>
> That's because you are forking, not branching.
> 
> If you make a branch (which can be considered a deliberate fork) then fossil will not nag you.
> 
> If you don't create a branch, Fossil fears you may be making a mistake by checking in two separate leafs from the same checkin, and does indeed warn you about a "fork" (accidental branch).
> 
> `fossil commit --branch my-new-branch-name`
> 
> will avoid the nagging and make your design choice explicit.
>
> (11) By Thomas Hess (luziferius) on 2024-03-21 23:39:36 in reply to 9 [link] [source]
>
> Entirely right.
> 
> Me mentioning this in the first place was because of OP's situation: If you have a git repository of some tool on a hosting platform like GitHub, and you forked the project (potentially multiple times), those GitHub forks are basically fancy feature branches under a different name. They still share the DAG of the common history.
>
> *But* on the git side, they use the same branch name for trunk. If OP now consolidates one or more of those git forks into a single fossil repository in order to be able to do merges in fossil, these git forks (in the GitHub definition of full project copy under different name) also naturally become fossil forks (in fossils definition of "fork"), with all that entails.
>
> I'm not sure if you can avoid this during the import.
> Fossil can rename the git main branch on import, but I have no clue what happens, if you import a git repository on default settings, and then import a related git repository with `--incremental`, while also overriding the default branch name on the secondary import. I'd have to test what happens in this situation.

и там дальше еще вопрос про клоны определенных бранчей, и секретного кода с доступом группы людей...
возможно, придётся пойти дальше бранча как tag property и сделать объект типа branchset ? inherit его from project-id - для полного аналога клона репы GitHub ?
===
19.06.24
к записи от 15.06.24: если объединять в каталоге и список файлов, и список коммитов, не получится ли воспроизведение проблемы масштабирования fossil - у него же манифест в каждом коммите, а тут данные каталога опять индивидуальны на каждый коммит вместо неизменных tree object в git
может быть (надо еще подумать), но не совсем, это ведь ограничено каталогом, а не полным деревом репозитория
при раздельных структурах на список "имя => инода" и на массив "инода => коммит" получается как JOIN таблиц в SQL, по-своему тоже красиво
правда, и с вытекающими затратами на этот join - денормализация в column-oriented делается не зря
вообще, представить pb catalog не просто как "ключ => значение", а как таблицу - может быть полезно даже и при хранении коммитов вовне, т.е. независимая идея (мало ли что может понадобиться, ACL например)
вообще, с точки зрения не только NNTP overview, но и интерфейса а-ля GitHub - в каталоге таки удобно индексировать инфу, чтоб не делать условный stat() - размер и дату например
хотя, при представлении единой структурой pb table/pb catalog и JOIN, оно же может лежать и в соседней таблице "инода => коммит"
какой размер дать записи? в ZAP это uint16_t la_next для 21-byte chunks, то есть 1376256 байт теоретический максимум на value
===
20.06.24
что, если применить таки SQLite для pb catalog, но не целиком (256 Тб слишком мало), а для индексной части?
а как? длинные не сунешь, uint64=>uint64, а что туда? как свернуть алфавит в 64 бита?
всё-таки RB tree, но что, если записи фиксированной длины, а и key, и value вынести в slab? оставить только смещение в нём, ну или прямо, если <=8 байт
к записи от 29.04.24: засовывать ли это всё в API getdirentries() ? например, раз 2^56, то 256 вариантов - каждый на столбец? тогда у каждого автоматически ограничение размера как у длины имени
sys/sys/dirent.h:#define        MAXNAMLEN  255
sys/i386/ibcs2/ibcs2_dirent.h:#define IBCS2_MAXNAMLEN              512
===
22.06.24
до 256 столбцов и произвольные по ним индексы вроде одобрил
страницы переменной длины чем-то не катят, надо вспомнить чем
- вероятно тем, что если getdirentries() записи по 1 байту, то произвольный lseek() должен как-то найти запись без хранения итератора - а как, если не хранить оглавление
  - не, это не то, ведь при переменной длине prefix tree как раз по записям произвольного размера, на то и переменная длина страницы
  - с оглавлением тоже есть проблема, это ведь 2^59 байт в случае 2^56 записей, целая 1/32-я файла
- может, сложностью расчета размеров меты, сколько качать?
- необходимостью изобретать отдельный формат хранения для этих переменных длин, с теми же проблемами? на диске / в SQLite блоки/страницы всё-таки фиксированной длины
- может, размером изменений? если 256 записей были по мегабайту, изменение одного байта потребует перекачивать порядка 256 Мб вместо пары-тройки небольших страниц

тогда RB TREE entry будут занимать не 21, а 24 байта - полноценные смещения, и отдельно байты битмапа red/black на все индексы
и максимальный размер зависит от размера slab, то есть от размера страницы
если страницы указателей на страниц на чисто смещения, то добавить sparse (из чисто нулей) и надо оценить, сколько на 2^64 надо уровней - зависит от размера указателя, то есть хэша
для 32 байт и 8 Кб будет 256 на странице, для 2^64 выходит 8 уровней? не многовато ли
а не, на нижнем уровне 8 Кб сама = 2^13, т.е. следующий уровень 8+13=21 (2 Мб)
значит N + L*(N-5) = 64 и ровных решений тут нет, 2^17 ближайшее - для 65 бит, и 3 уровня page table всего... но 128 Кб - как-то многовато
при неровной длине хэша будет обобщенное неравенство 2^N * (2^N/H)^L >= 2^64
ну кстати да, ведь по пути еще желательно мету записывать - например, о числе свободных слабов

но вообще этот способ только для блочно-страничных файлов годится - для остальных же как-то паковать диффы надо, git pack, fossil delta...
вероятно, нужны неравномерные интервалы (тут старое, тут измененное)), тогда и вопрос размера страницы/указателя не стоит в общем случае
===
24.06.24
общий формат каталога: наверное нужно делать поначалу как страницах SQLite, а дальше как в UMA - типа вот kegs разного размера, мб и описывать через себя же
если разные размеры страниц, то в её заголовок нужен nonce и checksum (как в sqlite journal), чтоб уменьшить вероятность совпадения с данными в slab - на случай recovery
или не совсем любые размеры, а как page table directory в amd64 делать?..
нельзя просто взять всю запись в CBOR и в slab и перемещать при изменении - ведь offset/rowid при итерации по getdirentries() не должен меняться
то есть получается, мини-заголовок, первый RB-индекс, uint8_t d_type, начало имени/смещение, uint64_t d_fileno или смещение - всегда в первой, остальное всегда в overflow slabs
причем uint16_t d_reclen задает верхнюю границу размера для имени = первого столбца, остальные можно сделать длиннее
тогда если разрешать индексы по другим полям, то они всегда в overflow, неудобно
сделать разные форматы записей? типа индекс - лишь один из них, и по имени - лишь один из индексов, первичный
- а что, это заодно унифицирует giant CBOR map и giant CBOR array - когда индекса по имени вообще нет, чисто массив
- блин, с чистым массивом проблема - как переместиться по голому индексу, опять оглавление на 512 Тб?
  - хм, ну это можно решить тем же аллокатором, не резервируя всю область - то
    есть иерархически, prefix tree, но не то, которое снаружи для смещений
    в файл, а внутреннее, развязать одно от другого
  - если какой-то столбец хранится в индексе и не скопирован в тело основного
    payload, то как его референсить при обращении по чисто rowid? придется
    вместо undef хранить смещение на запись индекса... типизированную CBOR tag?

типы записей - идентификатор в мини-заголовке:
'C' - columns (payload) start
'K' - key, индекс с RB
'I' - index of BLOB - смещения на middle и tail длинного блоба
'M' - middle of BLOB
'T' - tail of BLOB
'P' - prefix tree to get record offsets
не, надо еще типы страницы, и не пересекающиеся с ними, чтобы PB в нулевой странице, и возможность off-page bitmaps как в UMA
===
26.06.24
если делать LIST_REMOVE, то соседние страницы изменяются за просто так, чего для пересчета хэшей надо избегать (как можно меньше изменений)
значит в слабе минимум указателей
struct page_slab {
	uint8_t		ps_type;	/* id of page */
	uint8_t		ps_flags;	/* XXX */
	uint16_t        ps_freecount;	/* How many items are free? */
	uint16_t	ps_itemsize;	/* 0 if non-slab page */
	uint8_t		ps_first;	/* offset to first item */
	uint8_t		ps_reserved0;
	uint64_t	ps_keg;		/* 8: Keg we live in (file address) */
	char		ps_bitmap[240];	/* 16: XXX how many actually? */
};

а ведь проблема с prefix tree - на каждую запись это еще по 8 байт оверхеда указателя, надо бы на страницу
тогда на ней должно быть ровное количество 2^N итемов, но с тем же индексом по 48 байт - не выходит
распрощаться с align и сделать по 63 байта? тогда по 64 влезет на 4096 страницах
тогда префиксные страницы указателей на страницы по 9 бит и 7 байт указатель - блин, почти полкилобайта потеря
а если не префиксным? ведь указатели на rowid концептуально подобны номерам direct-блоков в inode для файла
то есть, при отсутствии sparse это связный список, который только растёт с конца либо с него же уменьшается
значит, сделать аналог индексации direct-блоков, см. в записях за начало апреля где pbmft_extentr_t
причем плюс в том, что на малых размерах достаточно нижнего уровня, путь из корня короче, индексные страницы верхних уровней добавляются лишь при росте

данные по длинам имён файлов в /boot:
Len     Count     %       Run%
  3         4  0.2%     0.2%
  4         3  0.2%     0.4%
  5        13  0.7%     1.0%
  6       165  8.5%     9.6%
  7       146  7.6%     17.2%
  8       173  9.0%     26.1%
  9       299  15.5%    41.6%
 10       219  11.3%    53.0%
 11       194  10.1%    63.0%
 12       166  8.6%     71.6%
 13       126  6.5%     78.1%
 14        76  3.9%     82.1%
 15        59  3.1%     85.1%
 16        45  2.3%     87.5%
 17        40  2.1%     89.5%
 18        12  0.6%     90.2%
 19        22  1.1%     91.3%
 20         2  0.1%     91.4%
 21         4  0.2%     91.6%
 22        20  1.0%     92.6%
 23        54  2.8%     95.4%
 24        50  2.6%     98.0%
 25        16  0.8%     98.9%
 26        18  0.9%     99.8%
 27         4  0.2%     100.0%

и с корня:
Len     Count   %       Run%
  1      4831  0.2%     0.2%
  2     33592  1.6%     1.8%
  3     31506  1.5%     3.3%
  4     29803  1.4%     4.7%
  5     53204  2.5%     7.3%
  6     55624  2.6%     9.9%
  7    113144  5.4%     15.3%
  8    167775  8.0%     23.2%
  9    356229  16.9%    40.1%
 10     96538  4.6%     44.7%
 11    114011  5.4%     50.1%
 12    210424  10.0%    60.1%
 13     68511  3.3%     63.4%
 14     63232  3.0%     66.4%
 15     63318  3.0%     69.4%
 16     50599  2.4%     71.8%
 17     50947  2.4%     74.2%
 18     77789  3.7%     77.9%
 19     37360  1.8%     79.6%
 20     34612  1.6%     81.3%
 21     29246  1.4%     82.7%
 22     25618  1.2%     83.9%
 23     27519  1.3%     85.2%
 24     24741  1.2%     86.4%
 25     23788  1.1%     87.5%
 26     22555  1.1%     88.6%
 27     16156  0.8%     89.3%
 28     17906  0.8%     90.2%
 29     19231  0.9%     91.1%
 30     15603  0.7%     91.8%
 31     11402  0.5%     92.4%
 32     11300  0.5%     92.9%
 33     16555  0.8%     93.7%
 34     24368  1.2%     94.9%
 35      6359  0.3%     95.2%
 36      5810  0.3%     95.4%
 37      4340  0.2%     95.6%
 38      6020  0.3%     95.9%
 39      3536  0.2%     96.1%
 40     16463  0.8%     96.9%
 41      2367  0.1%     97.0%
 42      2491  0.1%     97.1%
 43      2380  0.1%     97.2%
 44      7675  0.4%     97.6%
 45      1460  0.1%     97.7%
 46      1495  0.1%     97.7%
 47      1384  0.1%     97.8%
 48      1404  0.1%     97.9%
 49       984  0.0%     97.9%
 50      1031  0.0%     97.9%
 51       723  0.0%     98.0%
 52       756  0.0%     98.0%
 53       878  0.0%     98.1%
 54       627  0.0%     98.1%
 55       543  0.0%     98.1%
 56       599  0.0%     98.1%
 57       671  0.0%     98.2%
 58       895  0.0%     98.2%
 59       944  0.0%     98.3%
 60       803  0.0%     98.3%
 61       486  0.0%     98.3%
 62       524  0.0%     98.4%
 63       662  0.0%     98.4%
 64       549  0.0%     98.4%
 65       454  0.0%     98.4%
 66       370  0.0%     98.4%
 67     29073  1.4%     99.8%
 68       287  0.0%     99.8%
 69       192  0.0%     99.8%
 70       184  0.0%     99.9%

то есть минимальную запись всё-таки надо расширять, 8 байт явно недостаточно - больше всего с длиной 9, и большинство файлов требуют 20
а еще же типичное применение - адрес коммита в value, а это под 39 байт может вырасти: CBOR Tag, CBOR Major, дата/время и бранч...
отвести несколько битов флагов - под длину take common prefix of value из описания индекса? а там держать tag/major/len
а стоп, при разделении в отдельный атрибут будет же не коммит в каталоге, а UUID иноды - а это всего 16 байт, 20 с длиной и тэгом
хотя для соседнего атрибута с коммитами безиндексные rows тоже получается должны иметь сходный размер, раз аж в 2 раза больше
===
26.06.24
аналог direct-блоков - значит нужен дополнительный заголовок страницы по её типу, для next/prev/parent в данном случае
с деревьями страниц для самого аллокатора что-то не очень вырисовывается - как с дырками во freelist быть, например? перепаковывать массивы между страницами?
вариант: сделать страницу-описатель N страниц через каждые N+1 страниц, внутри её item'ы - уже LIST_ENTRY всех нужных списков
типов аллокаций же наверняка будет мало, значит можно уменьшить размер указателя на keg, держа только в первых сколько-то страницах

задача начинает напоминать файловую систему, то есть в PBMFS было похоже - а что, если по аналогии и сделать?
то есть, делаем размер блока ("кластера") условно, скажем, 64 байта - а записи индексов, rows и т.д. будут "файлы
ну допустим, но если Pyramint5 даже - это 16384 в самом большом куске внутри отсека, то есть аж 1 Мб
а характер данных, в отличие от SQLite, неизвестен - может они все будут маленькие?..
===
27.06.24
что, если сделать разного размера страницы внутри 1 мегабайта? типа, 0x8.... по 8 Кб и дальше по Хаффману
посмотреть SQLite4 wiki - хотя они похоже сами не определились, какой формат overflow лучше... может спросить на их форуме?
===
29.06.24
что, если таки уменьшить указатель RB до 7 байт - номер rowid? плюс: больше места для ключа, аж 1/8-я
минус: лукап получается не сразу, а надо держать индекс "rowid => адрес" теперь и для каждого индекса тоже
- что, если распологать страницы индекса и основные друг за другом? тогда и на
  value можно не держать ссылки, еще больше места для ключа?
  - тогда нельзя на ходу добавить или убавить индексов...
насколько всё проще, если бы индексы были в отдельном стриме с отдельными смещениями... 4096-21*195=1, 4096-21*192=64
а ведь кстати да, резольвинг оффсета - это ведь тоже неявные затраты по дереву хэшей
всё-таки хотя бы префикс ключа надо оставить в индексе, иначе дорого
и, видимо, делить на несколько сегментов на уровне дерева страниц хэшей по Merkle tree, иначе общее плоское пространство оффсетов дорого
===
30.06.24
равное дерево выглядит не очень для маленьких размеров, взять схему direct/indirect-блоков в обычной fs?
по аналогии с кодом UFS, если на наш адрес отвести 64 байта в имплементации (sha-384 и тип хэша, 8 байт на адрес локального блока, инфа о левеле, числе свободных):

bsize=32768; adrsz=64; nindir=bsize/adrsz
(12+nindir+nindir^2+nindir^3+nindir^4+nindir^5)*bsize/2^50
1026.00391389464493840932

bsize=65536; adrsz=64; nindir=bsize/adrsz
(12+nindir+nindir^2+nindir^3+nindir^4+nindir^5)*bsize/2^50
65600.06256109545938670635

что-то пока склоняюсь к тому, чтобы хранить индексы прям рядом с value, допустим 192 байта если два индекса
но это запрет ALTER TABLE, точнее CREATE INDEX, после создания...
типа 24 байта постоянных, из них 3 байта: 16 бит длина ключа, 1 бит red/black, 1 бит присутствия overfow offset и 6 бит присутствующей длины (без overfow offset)
можно ли без флага offset?
- можно, если длину overflow offset как 3 бита, от 0 до 7 байт, позволит для
  коротких файлов тратить меньше на смещение

а что, если двумерный массив страниц - из главной value-страницы ссылка на соответствующую страницу индексов? вот прям 32 байта хэша
всё равно же Merkle tree нужен, так что только в одном направлении можно - и значит увеличение числа страниц при обновлении
===
01.07.24
к концу записи от 22.06.24: всё-таки поглядеть на IPFS и отвязать хранение хэшей от страниц?
https://raw.githubusercontent.com/ipfs/papers/master/ipfs-cap2pfs/ipfs-p2p-file-system.pdf 3.6.7 "data": ["tree", "blob", "tree", "list", "blob" "blob"]
именно что для возможности удобства диффов они это сделали, но тогда:
- всё равно нужна локализация изменений ближе к друг другу, даже если блоки неровные
- нужна структура для индексации таких неровных блобов для быстрого поиска нужного offset
  - напоминает проблему индексации столбцов внутри одного row
- а хранить-то мы это всё равно будем в файловой системе, где блоки фиксированного размера... большой tail-packing?

для юзания в файловой системе - надо зарезервировать "локальный столбец", который при хэшировании не учитывается - для номера иноды например

ptrmap: 8-байтные оффсеты на владеющий slab или 0 для free, т.е. 512 страниц при 4096 странице

slab:
uint64_t: page number in zone (for reverse from offset) - zone number - flags?
uint64_t: first item address

в "UMA" Zone держать дерево оффсетов на каждую страницу в зоне - это получается аналог mft_addrl/mft_addri
в записях за 05.04.24 и 09.04.24 забыл записать высчитанное max число уровней для MFT, ну рассчитаем его тут заново
поскольку в pbmfs экстенты могли быть большие, а хотелось скомпрессить, там сумму считать надо - здесь же можно в I-странице сделать пары uint64_t=>uint64_t "оффсет в зоне => адрес в файле"
это позволит в сортированном блоке использовать половинное деление - ускорение, если страница большая
значит для страницы 2^P байт, 64-P бит на номер страницы и P бит на число блоков в экстенте - для L-страниц, одна такая описывает минимум 2^(P-3)*PAGESIZE байт
грубо, при максимальной фрагментации, 2^(P-4) записи в I-странице - для 1 уровня (указатели на L-страницы) охватывает 2^(P-4 + P-3) страниц зоны = 2^(2*P-7+P) байт
для 2 уровня еще 2^(P-4) = 2^(4*P-11) байт, обобщаем L*P - 4*L + 2*P - 3 = 62, если лимит для зоны 2^62 байт
не выходит ровно... L=2 P=19 или L=4 P=14 могло бы для 2^65
а вообще, P=15 L=3 дают 2^60, но это сколько может адресовать одна корневая страница - а их можно четыре массивом прямо в зоне как дополнительный уровень
при 32768 байт на страницу это ж сколько потребуется битов на слабы...
а еще оверхед на сами страницы аллокатора... придется делать видимо не 2, а 3 формата: full, tiny < 4096 и small меньше... скольки?
а пусть будет как раз по дефолтному размеру блока UFS2 и фрагменту 4096 - фрагменты там по direct-блокам, т.е. первые 12
#define UFS_NDADDR      12
хотя 96 даже маленьких слабов в одну только первую страницу не влезут... ну можно разные пороги для увеличения и уменьшения
и всё-таки, с оффсетами допустим решил, а поиск свободных? всё-таки LIST_ENTRY как в UMA ?
===
02.07.24
скомбинировать дерево оффсетов - с инфой о свободных, вместо LIST_ENTRY ?
но ведь оно не меняется при аллокации итемов
а еще сами слабы - это же как нижний уровень дерева оффсетов (mft_addrl), ведь в слабе есть указатель на первый адрес, может, скомбинировать?
а если один slab адресует несколько страниц подряд?
- можно запретить
- или вписать число в flags
- или пофигу? на уровне выше же есть каунты, точнее оффсеты

вообще не совсем оффсеты, страница со слабами же предполагалась описывать несколько следующих страниц, а они могут быть разных типов (зон)
с другой стороны, если на странице все слабы из одной зоны, то им можно
1) дать точный размер под битмап итемов именно этой зоны, если делать разными
   - даже в UMA так не делают, под максимальный (256 итемов) всегда выделено
2) только тогда эффект уменьшения от изменения соседних LIST_ENTRY, ведь списки-то внутри одной зоны
3) придется иметь часть слабов свободными (нули?), но на малых размерах каталога - это по отдельной странице на каждую зону, оверхед
   - ну это другим форматом для малых размеров решаемо, но надо его еще придумать

если делать слабы разного размера (битмапов), то как тогда вообще рассчитывать размер дерева?.. формат-то у I-страниц не изменится, а вот число...
допустим, размер слаба одинаковый, тогда вчерашние расчеты, пусть слаб 64 байта - для L-страниц, одна такая описывает минимум 2^(P-6)*PAGESIZE байт
дальше снова 2^(P-4) для каждого I-уровня, для 1 уровня (указатели на L-страницы) охватывает 2^(P-4 + P-6) страниц зоны = 2^(2*P-10+P) байт
для 2 уровня еще 2^(P-4) = 2^(4*P-14) байт, обобщаем L*P - 4*L + 2*P - 6 = 62, если лимит для зоны 2^62 байт
дает решение для L=3 P=16 - но это здоровенная страница, и она в такой слаб на 64 байта точно не влезет
P-6 для 4096 страницы, т.е. 12-6, т.е. всегда 64 слаба, что ли, независимо от размера страницы
ну пусть, тогда для 1 уровня (указатели на L-страницы) охватывает 2^(P-4 + 6) страниц зоны = 2^(P+2) байт
то есть L*P + 2 = 62, если лимит для зоны 2^62 байт - это 5 уровней 4096-байтных страниц или 4 на 32768
512 байт на слаб? многовато... а, это из расчета на 8-байтные элементы либо когда 32 байта на битмап было реально (остальное указатели, в 4096)

а вопрос с LIST_ENTRY (то есть совать в дерево оффсетов или нет) можно же просто посчитать - сколько это изменений страниц в лучшем и худшем случаях?
посмотрим релевантный код UMA:
/*
 * Find a slab with some space.  Prefer slabs that are partially used over those
 * that are totally full.  This helps to reduce fragmentation.
keg_first_slab(uma_keg_t keg, int domain, bool rr)
	...
	do {
		dom = &keg->uk_domain[domain];
		if (!LIST_EMPTY(&dom->ud_part_slab))
			return (LIST_FIRST(&dom->ud_part_slab));
		if (!LIST_EMPTY(&dom->ud_free_slab)) {
			slab = LIST_FIRST(&dom->ud_free_slab);
			LIST_REMOVE(slab, us_link);
			LIST_INSERT_HEAD(&dom->ud_part_slab, slab, us_link);
			return (slab);
		}
		...
	} while (domain != start);

slab_alloc_item(uma_keg_t keg, uma_slab_t slab)
	...
	freei = BIT_FFS(SLAB_SETSIZE, &slab->us_free) - 1;
	BIT_CLR(SLAB_SETSIZE, freei, &slab->us_free);
	item = slab->us_data + (keg->uk_rsize * freei);
	slab->us_freecount--;
	keg->uk_free--;

	/* Move this slab to the full list */
	if (slab->us_freecount == 0) {
		LIST_REMOVE(slab, us_link);
		dom = &keg->uk_domain[slab->us_domain];
		LIST_INSERT_HEAD(&dom->ud_full_slab, slab, us_link);
	}

	return (item);

slab_free_item(uma_keg_t keg, uma_slab_t slab, void *item)
	...
	/* Do we need to remove from any lists? */
	if (slab->us_freecount+1 == keg->uk_ipers) {
		LIST_REMOVE(slab, us_link);
		LIST_INSERT_HEAD(&dom->ud_free_slab, slab, us_link);
	} else if (slab->us_freecount == 0) {
		LIST_REMOVE(slab, us_link);
		LIST_INSERT_HEAD(&dom->ud_part_slab, slab, us_link);
	}

	/* Slab management. */
	freei = ((uintptr_t)item - (uintptr_t)slab->us_data) / keg->uk_rsize;
	BIT_SET(SLAB_SETSIZE, freei, &slab->us_free);
	slab->us_freecount++;

	/* Keg statistics. */
	keg->uk_free++;

head1      before1       our_slab        after1
next->  <-prev next->  <-prev next->  <-prev next->

head2      before2       our_slab        after2
next->  <-prev next->  <-prev next->  <-prev next->

LIST_REMOVE:      before1.next = &after1; after1.prev = &before1
LIST_INSERT_HEAD: head2.next = &our_slab; our_slab.next = &before2; before2.prev = &head

значит трогаются головы, два элемента в старом, один элемент в новом, и наш оперируемый - он с головами в любом случае, не в счёт
следовательно, кроме них, дополнительно, от 0 в лучшем (все 4 элемента на одной странице) до 3 страниц в худшем случае

а в случае дерева какая структура данных вообще? как искать и обновлять?
ну наверное на I-странице в младших битах записать 3 флага - имеются ud_free_slab, ud_part_slab, ud_full_slab
а на L-странице как? если объединены с деревом оффсетов, то сканить все слабы?
если не кратный 2^N размер / заголовок страницы, можно в нём три битмапа, например
а еще в этот заголовок, если объединены с оффсетом, указатель на родительскую I-страницу, и на каждой из них тоже
но при поиске это скан нижного уровня с начала всего? а если их очень много?
===
03.07.24
так много усилий в этот slab-аллокатор... может его не только для каталогов тогда применить как нижний уровень, а и для других объектов?
в ptrmap для страниц дерева оффсетов можно делать указатель прям внутрь структуры - типа какому именно списку принадлежит страница

struct pb_list_head_t {
	uint64_t	pbl_first;	/* Address (offset in file) */
	uint64_t	pbl_last;	/* Address (offset in file) */
};

struct pbz_ilist_t {
	struct pb_list_head_t pzil_head;
	uint64_t	pzl_second_ofs;	/* First offset described by second block (sum of first block) */
}

#define PB_ZONE_ILEV_MAX 5
struct pb_zone {
	char		pz_magic;	/* 'Z' */
	char pz_name[31];		/* No NUL if all 31 */

	/* Settings. */
	uint16_t	pz_flags;	/* Alignment is here */
	uint16_t	pz_pgoff;	/* Offset to uma_slab struct XXX */
	uint16_t	pz_ppera;	/* pages per allocation from backend XXX */
	uint16_t	pz_ipers;	/* Items per slab */
	uint32_t	pz_size;	/* Requested size of each item XXX 16 bits? */
	uint32_t	pz_rsize;	/* Real size of each item (adjusted for alignment) */
	uint64_t	pz_maxpages;	/* Maximum number of pages to alloc */

	/* Stats. */
	uint64_t	pz_allocs;	/* Total number of allocations - for AUTOINCREMENT */
	uint64_t	pz_fails;	/* Total number of alloc failures XXX */
	uint64_t	pz_frees;	/* Total number of frees */
	uint64_t	pz_userpages;	/* Total data page count */
	uint64_t	pz_metapages;	/* Slabs and tree - all overhead pages */
	uint64_t	pz_free;	/* Count of items free in slabs */
	uint64_t	pz_reserve[2];	/* Reserved */

	/* Used for handling user allocations. */
	struct pb_list_head_t	pz_part_slab;	/* partially allocated slabs */
	struct pb_list_head_t	pz_free_slab;	/* empty slab list */
	struct pb_list_head_t	pz_full_slab;	/* full slabs */

	/* Pages list. */
	struct pb_list_head_t	pz_dhead;	/* Pages with slabs itself */
	struct pbz_ilist_t	pz_ihead[PB_ZONE_ILEV_MAX];
};
быстро 256 байт набралось и больше, как бы не пришлось статистику выкинуть и second_ofs в I-головах

4096%288 = 64	- 14 итемов, 18 страниц и 4 в головной: 288*4+64*18 = 2304
4096%272 = 16	- 15 итемов, 17 страниц и 1 в головной: 272+64*17 = 1360
4096%312 = 40	- 13 итемов, 19 страниц и 9 в головной: 312*9+19*64 = 4024 ! нехватка на клиентский заголовок
ну поскольку в головной странице места вагон, видимо первый вариант
и еще в головной предусмотреть массив "виртуальных" страниц - пока влезает (остальное нули), читать из неё, так сэкономить на первой ptrmap и пока-еще-однослабных зонах
или нет, не усложнять формат, всё равно будет еще tiny-формат
===
04.07.24
encoding заголовка тела - для однобайтных равен соответствующему байту CBOR, дальше для малых скаляров как отрезанный первый байт CBOR, для чисел 28..31 дополнительные длины 3,5,6,7
у строк резервируем, в 0xFx оставляем два кода для нулевых массивов и map'ов, и наконец диапазон 0x80..0xef кодирует длину блоба (т.к. тег одиночный невозможен, например, и т.д.):
10aaaaaa          - 0..63
110bbbbb bbbbbbbb - 8256 значений
1110cccc cccccccc cccccccc - 1 мегабайт TBD прибавлять ли те 8256 ?
F2 - null array (0x80), F3 - null map (0xa0)
reserve 5B/5C/5E, 7B/7C/7E, FB/FC/FE to not clash with CBAR
TBD F0 & F1 possible, use them for >1 Mb?
- это от формата индексации зависит...:
а ведь не только индексация столбцов нужна - при их изменении еще не переписывать все мегабайты, то есть для каждого отдельно делать суб-"файл" с инодой? и tail packing...
значит один код на "столбец внешний"
===
05.07.24
при превышении 64 просто пишем указатель, и это получается число, сколько там - для 8192 до 2 страниц, до мегабайта - на страницу с 256 указателями на страницы
TBD делать ли больше мегабайта? совмещать ли с CBAR ? еще, что делать со строками длиннее 23, т.е. для чисел 24..37 ? там же CBOR уже кодирует длину
с точки зрения ровного размера для расчета числа столбцов - лучше бы ограничить на целиком 64, а не "код + 64 байта начала + 8 байт указателя"
унифицировать с overflow у ключа? нужно поддержать в slab несколькостраничные аллокации

в индексе - копию начала value и флаг truncated? и local reserved

256/23 = 11.13
256%23 = 3
то есть 0..22 байта копии value и 0..10 байт local reserved
но если UUID займет с CBOR 20 байт, и еще 8 на иноду - это (после 4 заголовка и 24 RB) остается всего 8 байт на ключ, включая указатель overflow - то есть ничего!
значит, объединить типовой каталог в сетевом варианте без отдельных value - при слабе 64 байта невозможно
можно поднять до 80 байт, но это будет иметь смысл только если на весь каталог поставлено ограничение размера value - что нет отдельной зоны под него, тогда 80 байт < 128
===
08.07.24
для описания группы ptrmap-страниц нужен не битмап, а по 1 байту: каждый бит, что в такой-то ptrmap-странице доступны contiguous области по 1 странице, по 2, и т.д. - для удобства поиска больших кусков при аллокации
это всего 7-8 если линейно, причем последующий включает в себя предыдущий - что-то можно оптимизировать?
наверное надо считать их как независимые группы, то есть ставить 5 и 6 например если два изолированных куска по 5 и 6 либо один по 11
===
10.07.24
не забыть о необходимости частичной доступности каталога по сети - как в IMAP-папке или NNTP
===
27.07.24
https://git-scm.com/docs/gitnamespaces чем-то похоже на volume, только костыль и без security
https://fossil-scm.org/forum/info/0e3b65df92417c168d05 в ту же тему всё обсуждение, e.g. Corrollary:
<<Yes, for a reason sufficiently simple that I'm willing to treat it as an axiom of software development: if two different directories contain files with independent lifetimes, they need to live in independent repos.

Corrollary: The only time two or more files should live in a single repo is when they need to be versioned in lockstep.

This does not always occur here, but that's because it's an ideal I strive to uphold, not a law I'm bound by. The only penalties I pay for my failure to achieve the ideal are personal.

The primary exception is "junk drawer" repos, like the one for the locally-written contents of `~/bin`. Each file has an independent lifetime, for the most part, since few of these programs interoperate, or even, for that matter, *cooperate*. Yet, I put them all together because I want them checked out together and updated together, in a single step.>>
===
12.10.24
хранить в иноде (основной fs) некую статистику по файлу, чтобы знать, как его лучше реаллоцировать - например, из какого слаба в какой при мелком хвосте, или ухода из слабов
===
30.10.24
TODO look at https://steveklabnik.github.io/jujutsu-tutorial/introduction/introduction.html

